{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Structured data learning with TabTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This example demonstrates how to do structured data classification using [TabTransformer](https://arxiv.org/pdf/2012.06678.pdf), a deep tabular data modeling architecture for supervised and semi-supervised learning. The TabTransformer is built upon self-attention based Transformers. The Transformer layers transform the embeddings of categorical features into robust contextual embeddings to achieve higher predictive accuracy.\n",
    "\n",
    "**TabTransformer: Tabular Data Modeling Using Contextual Embeddings (2020 arXiv)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T08:26:18.582687Z",
     "start_time": "2022-02-19T08:26:18.576124Z"
    }
   },
   "source": [
    "<img src='./images/tabtransformer.png' width='400'> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T10:22:58.737318Z",
     "start_time": "2022-02-19T10:22:58.591593Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import einsum\n",
    "from einops import rearrange\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from urllib import request\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:08:57.648914Z",
     "start_time": "2022-02-19T09:08:57.620113Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- Dataset : United States Census Income Dataset\n",
    "- Info\n",
    "    - Provided by the UC Irvine Machine Learning Repository\n",
    "    - Binary classification to predict whether a person is likely to be making over USD 50,000 a year\n",
    "    - 48,842 instances with 14 input features(5 numerical features, 9 categorical features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:08:57.916123Z",
     "start_time": "2022-02-19T09:08:57.739474Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder already exists.\n",
      "train_data.csv already exists.\n",
      "train_data load complete!\n",
      "test_data.csv already exists.\n",
      "test_data load complete!\n",
      "Train dataset shape: (32561, 15)\n",
      "Test dataset shape: (16281, 15)\n"
     ]
    }
   ],
   "source": [
    "CSV_HEADER = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education_num\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"gender\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "    \"native_country\",\n",
    "    \"income_bracket\",\n",
    "]\n",
    "\n",
    "annotation_folder = os.path.abspath(\".\") + \"/dataset/USCI\"\n",
    "\n",
    "train_data_file = os.path.join(annotation_folder, \"train_data.csv\")\n",
    "test_data_file = os.path.join(annotation_folder, \"test_data.csv\")\n",
    "\n",
    "if not os.path.exists(annotation_folder):\n",
    "    os.makedirs(annotation_folder)\n",
    "    print('Folder creation complete!')\n",
    "else:\n",
    "    print('The folder already exists.')\n",
    "\n",
    "if not os.path.isfile(train_data_file):\n",
    "    train_data_url = (\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "    )\n",
    "    train_data = pd.read_csv(train_data_url, header=None, names=CSV_HEADER)\n",
    "    train_data.to_csv(train_data_file, index=False)\n",
    "    print('train_data.csv creation complete!')\n",
    "else:\n",
    "    train_data = pd.read_csv(train_data_file)\n",
    "    print('train_data.csv already exists.')\n",
    "print('train_data load complete!')\n",
    "\n",
    "\n",
    "if not os.path.isfile(test_data_file):\n",
    "    test_data_url = (\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
    "    )\n",
    "    test_data = pd.read_csv(test_data_url, header=None, names=CSV_HEADER)\n",
    "    test_data = test_data[1:]\n",
    "    test_data.income_bracket = test_data.income_bracket.apply(\n",
    "        lambda value: value.replace(\".\", \"\")\n",
    "    )\n",
    "    test_data.to_csv(test_data_file, index=False)\n",
    "    print('test_data.csv creation complete!')\n",
    "else:\n",
    "    test_data = pd.read_csv(test_data_file)\n",
    "    print('test_data.csv already exists.')\n",
    "print('test_data load complete!')\n",
    "\n",
    "print(f\"Train dataset shape: {train_data.shape}\")\n",
    "print(f\"Test dataset shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:08:57.952937Z",
     "start_time": "2022-02-19T09:08:57.917768Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income_bracket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education_num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital_status          occupation    relationship    race   gender  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital_gain  capital_loss  hours_per_week  native_country income_bracket  \n",
       "0          2174             0              40   United-States          <=50K  \n",
       "1             0             0              13   United-States          <=50K  \n",
       "2             0             0              40   United-States          <=50K  \n",
       "3             0             0              40   United-States          <=50K  \n",
       "4             0             0              40            Cuba          <=50K  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define dataset metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:08:58.049244Z",
     "start_time": "2022-02-19T09:08:57.954557Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# A list of the numerical feature names.\n",
    "NUMERIC_FEATURE_NAMES = [\n",
    "    \"age\",\n",
    "    \"education_num\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "]\n",
    "# A dictionary of the categorical features and their vocabulary.\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \"workclass\": sorted(list(train_data[\"workclass\"].unique())),\n",
    "    \"education\": sorted(list(train_data[\"education\"].unique())),\n",
    "    \"marital_status\": sorted(list(train_data[\"marital_status\"].unique())),\n",
    "    \"occupation\": sorted(list(train_data[\"occupation\"].unique())),\n",
    "    \"relationship\": sorted(list(train_data[\"relationship\"].unique())),\n",
    "    \"race\": sorted(list(train_data[\"race\"].unique())),\n",
    "    \"gender\": sorted(list(train_data[\"gender\"].unique())),\n",
    "    \"native_country\": sorted(list(train_data[\"native_country\"].unique())),\n",
    "}\n",
    "\n",
    "# Name of the column to be used as instances weight.\n",
    "WEIGHT_COLUMN_NAME = \"fnlwgt\"\n",
    "# A list of the categorical feature names.\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
    "# A list of all the input features.\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "# A list of column default values for each feature.\n",
    "COLUMN_DEFAULTS = [\n",
    "    0.0 if feature_name in NUMERIC_FEATURE_NAMES + [WEIGHT_COLUMN_NAME] else \"NA\"\n",
    "    for feature_name in CSV_HEADER\n",
    "]\n",
    "# The name of the target feature.\n",
    "TARGET_FEATURE_NAME = \"income_bracket\"\n",
    "# A list of the labels of the target features.\n",
    "TARGET_LABELS = [\" <=50K\", \" >50K\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:08:58.130238Z",
     "start_time": "2022-02-19T09:08:58.052008Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'workclass': [' ?',\n",
       "  ' Federal-gov',\n",
       "  ' Local-gov',\n",
       "  ' Never-worked',\n",
       "  ' Private',\n",
       "  ' Self-emp-inc',\n",
       "  ' Self-emp-not-inc',\n",
       "  ' State-gov',\n",
       "  ' Without-pay'],\n",
       " 'education': [' 10th',\n",
       "  ' 11th',\n",
       "  ' 12th',\n",
       "  ' 1st-4th',\n",
       "  ' 5th-6th',\n",
       "  ' 7th-8th',\n",
       "  ' 9th',\n",
       "  ' Assoc-acdm',\n",
       "  ' Assoc-voc',\n",
       "  ' Bachelors',\n",
       "  ' Doctorate',\n",
       "  ' HS-grad',\n",
       "  ' Masters',\n",
       "  ' Preschool',\n",
       "  ' Prof-school',\n",
       "  ' Some-college'],\n",
       " 'marital_status': [' Divorced',\n",
       "  ' Married-AF-spouse',\n",
       "  ' Married-civ-spouse',\n",
       "  ' Married-spouse-absent',\n",
       "  ' Never-married',\n",
       "  ' Separated',\n",
       "  ' Widowed'],\n",
       " 'occupation': [' ?',\n",
       "  ' Adm-clerical',\n",
       "  ' Armed-Forces',\n",
       "  ' Craft-repair',\n",
       "  ' Exec-managerial',\n",
       "  ' Farming-fishing',\n",
       "  ' Handlers-cleaners',\n",
       "  ' Machine-op-inspct',\n",
       "  ' Other-service',\n",
       "  ' Priv-house-serv',\n",
       "  ' Prof-specialty',\n",
       "  ' Protective-serv',\n",
       "  ' Sales',\n",
       "  ' Tech-support',\n",
       "  ' Transport-moving'],\n",
       " 'relationship': [' Husband',\n",
       "  ' Not-in-family',\n",
       "  ' Other-relative',\n",
       "  ' Own-child',\n",
       "  ' Unmarried',\n",
       "  ' Wife'],\n",
       " 'race': [' Amer-Indian-Eskimo',\n",
       "  ' Asian-Pac-Islander',\n",
       "  ' Black',\n",
       "  ' Other',\n",
       "  ' White'],\n",
       " 'gender': [' Female', ' Male'],\n",
       " 'native_country': [' ?',\n",
       "  ' Cambodia',\n",
       "  ' Canada',\n",
       "  ' China',\n",
       "  ' Columbia',\n",
       "  ' Cuba',\n",
       "  ' Dominican-Republic',\n",
       "  ' Ecuador',\n",
       "  ' El-Salvador',\n",
       "  ' England',\n",
       "  ' France',\n",
       "  ' Germany',\n",
       "  ' Greece',\n",
       "  ' Guatemala',\n",
       "  ' Haiti',\n",
       "  ' Holand-Netherlands',\n",
       "  ' Honduras',\n",
       "  ' Hong',\n",
       "  ' Hungary',\n",
       "  ' India',\n",
       "  ' Iran',\n",
       "  ' Ireland',\n",
       "  ' Italy',\n",
       "  ' Jamaica',\n",
       "  ' Japan',\n",
       "  ' Laos',\n",
       "  ' Mexico',\n",
       "  ' Nicaragua',\n",
       "  ' Outlying-US(Guam-USVI-etc)',\n",
       "  ' Peru',\n",
       "  ' Philippines',\n",
       "  ' Poland',\n",
       "  ' Portugal',\n",
       "  ' Puerto-Rico',\n",
       "  ' Scotland',\n",
       "  ' South',\n",
       "  ' Taiwan',\n",
       "  ' Thailand',\n",
       "  ' Trinadad&Tobago',\n",
       "  ' United-States',\n",
       "  ' Vietnam',\n",
       "  ' Yugoslavia']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CATEGORICAL_FEATURES_WITH_VOCABULARY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:08:58.194513Z",
     "start_time": "2022-02-19T09:08:58.135345Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'LEARNING_RATE': 0.001,\n",
    "    'WEIGHT_DECAY': 0.0001,\n",
    "    'DROPOUT_RATE': 0.2,\n",
    "    'BATCH_SIZE': 265,\n",
    "    'NUM_WORKERS': 4,\n",
    "    'PIN_MEMORY' : True,\n",
    "    'NUM_EPOCHS': 15,\n",
    "    'NUM_TRANSFORMER_BLOCKS': 3,    # Number of transformer blocks.\n",
    "    'NUM_HEADS': 4,  # Number of attention heads.\n",
    "    'EMBEDDING_DIMS': 16,   # Embedding dimensions of the categorical features.\n",
    "    'MLP_HIDDEN_UNITS_FACTORS': [2, 1], # MLP hidden layer units, as factors of the number of inputs.\n",
    "    'NUM_MLP_BLOCKS': 2, # Number of MLP blocks in the baseline model.\n",
    "    'DEVICE': device\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Implement data reading pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:08:58.282493Z",
     "start_time": "2022-02-19T09:08:58.198811Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 dataframe,\n",
    "                 VOCABULARY=CATEGORICAL_FEATURES_WITH_VOCABULARY,\n",
    "                 FEATURE_NAMES=FEATURE_NAMES,\n",
    "                 NUMERIC_FEATURE_NAMES=NUMERIC_FEATURE_NAMES,\n",
    "                 TARGET_FEATURE_NAME=TARGET_FEATURE_NAME,\n",
    "                 TARGET_LABELS=TARGET_LABELS,\n",
    "                 WEIGHT_COLUMN_NAME=WEIGHT_COLUMN_NAME,\n",
    "                 COLUMN_DEFAULTS=COLUMN_DEFAULTS):\n",
    "\n",
    "        data = dataframe.copy().dropna()\n",
    "\n",
    "        for idx, feature_name in enumerate(FEATURE_NAMES):\n",
    "            # data[feature_name] = data[feature_name].fillna(COLUMN_DEFAULTS[idx])\n",
    "            if feature_name not in NUMERIC_FEATURE_NAMES:\n",
    "                data[feature_name] = data[feature_name].map(lambda x: VOCABULARY[feature_name].index(x))\n",
    "\n",
    "        data[TARGET_FEATURE_NAME] = data[TARGET_FEATURE_NAME].map(lambda x: TARGET_LABELS.index(x))\n",
    "        self.weights = np.array(data.pop(WEIGHT_COLUMN_NAME), dtype=np.float32)\n",
    "        self.targets = np.array(data.pop(TARGET_FEATURE_NAME), dtype=np.int)\n",
    "        self.data = np.array(data[FEATURE_NAMES])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.from_numpy(self.data[idx])\n",
    "        target_index = torch.as_tensor(self.targets[idx])\n",
    "        weight = torch.as_tensor(self.weights[idx])\n",
    "        return features, target_index, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:08:58.696868Z",
     "start_time": "2022-02-19T09:08:58.285629Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_data)\n",
    "test_dataset = CustomDataset(test_data)\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          shuffle=True,\n",
    "                          batch_size=params['BATCH_SIZE'],\n",
    "                          pin_memory=params['PIN_MEMORY'],\n",
    "                          num_workers=params['NUM_WORKERS'])\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         shuffle=False,\n",
    "                         batch_size=params['BATCH_SIZE'],\n",
    "                         pin_memory=params['PIN_MEMORY'],\n",
    "                         num_workers=params['NUM_WORKERS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:08:58.706477Z",
     "start_time": "2022-02-19T09:08:58.698609Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({' <=50K': 24720, ' >50K': 7841})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train_data[TARGET_FEATURE_NAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:08:58.794031Z",
     "start_time": "2022-02-19T09:08:58.707638Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({' <=50K': 12435, ' >50K': 3846})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(test_data[TARGET_FEATURE_NAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:09:02.925355Z",
     "start_time": "2022-02-19T09:08:58.797208Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([265, 13]) torch.Size([265])\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(train_loader))[0].shape, next(iter(train_loader))[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:09:03.291061Z",
     "start_time": "2022-02-19T09:09:02.928421Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_shape = next(iter(train_loader))[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Experiment 1: a baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/tab_baseline.png' width='1000'> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:09:03.300073Z",
     "start_time": "2022-02-19T09:09:03.293648Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class create_mlp(nn.Module):\n",
    "    def __init__(self, dims, dropout_rate, activation, normalization_layer):\n",
    "        super(create_mlp, self).__init__()\n",
    "        self.mlp_layers = nn.ModuleList()\n",
    "        dims_pairs = list(zip(dims[:-1], dims[1:]))\n",
    "        \n",
    "        for ind, (dim_in, dim_out) in enumerate(dims_pairs):\n",
    "            mlp = nn.Sequential(\n",
    "                nn.LayerNorm(dim_in, eps=1e-6) if normalization_layer=='LayerNorm'\n",
    "                else nn.BatchNorm1d(dim_in),\n",
    "                nn.Linear(dim_in, dim_out),\n",
    "                activation,\n",
    "                nn.Dropout(dropout_rate)\n",
    "            )\n",
    "            self.mlp_layers.append(mlp)\n",
    "        self.mlp_layers = nn.Sequential(*self.mlp_layers)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.mlp_layers(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:09:03.390304Z",
     "start_time": "2022-02-19T09:09:03.301597Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "create_mlp                               --                        --\n",
       "├─Sequential: 1-1                        [265, 13]                 --\n",
       "│    └─Sequential: 2-1                   [265, 13]                 --\n",
       "│    │    └─LayerNorm: 3-1               [265, 13]                 26\n",
       "│    │    └─Linear: 3-2                  [265, 13]                 182\n",
       "│    │    └─GELU: 3-3                    [265, 13]                 --\n",
       "│    │    └─Dropout: 3-4                 [265, 13]                 --\n",
       "==========================================================================================\n",
       "Total params: 208\n",
       "Trainable params: 208\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.06\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.06\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.07\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(create_mlp([13, 13], 0.01, nn.GELU(), 'LayerNorm'), input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:09:03.447195Z",
     "start_time": "2022-02-19T09:09:03.393368Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class create_baseline_model(nn.Module):\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 FEATURE_NAMES=FEATURE_NAMES,\n",
    "                 CATEGORICAL_FEATURES_WITH_VOCABULARY=CATEGORICAL_FEATURES_WITH_VOCABULARY, ):\n",
    "        super(create_baseline_model, self).__init__()\n",
    "        self.embedding_dims = params['EMBEDDING_DIMS']\n",
    "        self.num_mlp_blocks = params['NUM_MLP_BLOCKS']\n",
    "        self.mlp_hidden_units_factors = params['MLP_HIDDEN_UNITS_FACTORS']\n",
    "        self.dropout_rate = params['DROPOUT_RATE']\n",
    "        self.device = params['DEVICE']\n",
    "        \n",
    "        self.num_features = len(FEATURE_NAMES)\n",
    "        self.num_cat_features = len(CATEGORICAL_FEATURES_WITH_VOCABULARY)\n",
    "        self.num_numeric_features = self.num_features - self.num_cat_features\n",
    "        self.embedding_layers = nn.ModuleList()\n",
    "        self.mlp_blocks = nn.ModuleList()\n",
    "\n",
    "        for feature in FEATURE_NAMES:\n",
    "            if feature in CATEGORICAL_FEATURES_WITH_VOCABULARY.keys():\n",
    "                emb = nn.Embedding(len(CATEGORICAL_FEATURES_WITH_VOCABULARY[feature]), self.embedding_dims)\n",
    "                self.embedding_layers.append(emb)\n",
    "                \n",
    "        input_size = (self.embedding_dims * self.num_cat_features) + self.num_numeric_features\n",
    "\n",
    "        for blocks_idx in range(self.num_mlp_blocks):\n",
    "            block = create_mlp([input_size, input_size],\n",
    "                               self.dropout_rate,\n",
    "                               nn.GELU(),\n",
    "                               'LayerNorm')\n",
    "            self.mlp_blocks.append(block)\n",
    "        self.mlp_blocks = nn.Sequential(*self.mlp_blocks)\n",
    "        \n",
    "        mlp_hidden_units = [factor * input_size for factor in self.mlp_hidden_units_factors]\n",
    "        mlp_hidden_units.insert(0, input_size)\n",
    "\n",
    "        self.MLP = create_mlp(mlp_hidden_units,\n",
    "                              self.dropout_rate,\n",
    "                              nn.SELU(),\n",
    "                              'BatchNorm')\n",
    "\n",
    "        self.classifier = nn.Linear(mlp_hidden_units[-1], 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        numeric_vectors = []\n",
    "        for i in range(self.num_numeric_features):\n",
    "            emb = inputs[:, i].view(inputs.size(0), -1, 1).float().to(self.device)\n",
    "            numeric_vectors.append(emb)\n",
    "        numeric_vectors = torch.cat(numeric_vectors, dim=1)\n",
    "        numeric_vectors = numeric_vectors.squeeze()\n",
    "\n",
    "        categoical_vectors = []\n",
    "        for i in range(self.num_cat_features):\n",
    "            emb = self.embedding_layers[i](inputs[:, self.num_numeric_features+i].view(inputs.size(0), -1).long().to(self.device))\n",
    "            categoical_vectors.append(emb)\n",
    "        categoical_vectors = torch.cat(categoical_vectors, dim=2)\n",
    "        categoical_vectors = categoical_vectors.squeeze()\n",
    "        embeddings = torch.cat([numeric_vectors,categoical_vectors], dim=1)\n",
    "\n",
    "        features = embeddings\n",
    "        features = self.mlp_blocks(features)\n",
    "\n",
    "        features = self.MLP(features)\n",
    "        output = self.classifier(features)\n",
    "\n",
    "        return output.type(torch.FloatTensor).to(self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:09:03.544252Z",
     "start_time": "2022-02-19T09:09:03.450434Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "baseline_model = create_baseline_model(params).to(device)\n",
    "optimizer = optim.Adam(baseline_model.parameters(), lr=params['LEARNING_RATE'])\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:09:03.615775Z",
     "start_time": "2022-02-19T09:09:03.547228Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "create_baseline_model                    --                        --\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "│    └─Embedding: 2-1                    [265, 1, 16]              144\n",
       "│    └─Embedding: 2-2                    [265, 1, 16]              256\n",
       "│    └─Embedding: 2-3                    [265, 1, 16]              112\n",
       "│    └─Embedding: 2-4                    [265, 1, 16]              240\n",
       "│    └─Embedding: 2-5                    [265, 1, 16]              96\n",
       "│    └─Embedding: 2-6                    [265, 1, 16]              80\n",
       "│    └─Embedding: 2-7                    [265, 1, 16]              32\n",
       "│    └─Embedding: 2-8                    [265, 1, 16]              672\n",
       "├─Sequential: 1-2                        [265, 133]                --\n",
       "│    └─create_mlp: 2-9                   [265, 133]                --\n",
       "│    │    └─Sequential: 3-1              [265, 133]                18,088\n",
       "│    └─create_mlp: 2-10                  [265, 133]                --\n",
       "│    │    └─Sequential: 3-2              [265, 133]                18,088\n",
       "├─create_mlp: 1-3                        [265, 133]                --\n",
       "│    └─Sequential: 2-11                  [265, 133]                --\n",
       "│    │    └─Sequential: 3-3              [265, 266]                35,910\n",
       "│    │    └─Sequential: 3-4              [265, 133]                36,043\n",
       "├─Linear: 1-4                            [265, 1]                  134\n",
       "==========================================================================================\n",
       "Total params: 109,895\n",
       "Trainable params: 109,895\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 29.12\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 3.09\n",
       "Params size (MB): 0.44\n",
       "Estimated Total Size (MB): 3.55\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(baseline_model, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:09:03.662809Z",
     "start_time": "2022-02-19T09:09:03.618151Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:09:03.759244Z",
     "start_time": "2022-02-19T09:09:03.666290Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:09:03.828587Z",
     "start_time": "2022-02-19T09:09:03.762630Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_data, optimizer, loss_fn, use_fp16=True, max_norm=None, progress_display=False):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    Acc = AverageMeter('Acc', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_data),\n",
    "        [batch_time, losses],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    for idx, [x, y, w] in enumerate(train_data):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        input = x\n",
    "        target = y.unsqueeze(1).to(device, dtype=torch.float32)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=use_fp16):\n",
    "            predictions = model(input)\n",
    "            train_loss = loss_fn(predictions, target)\n",
    "        if use_fp16:\n",
    "            scaler.scale(train_loss).backward()\n",
    "            if max_norm is not None:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            train_loss.backward()\n",
    "            if max_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            optimizer.step()\n",
    "        acc = binary_acc(predictions, target)\n",
    "        Acc.update(acc, input.size(0))\n",
    "        losses.update(train_loss.item(), input.size(0))\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if progress_display == True and idx % 50 == 0:\n",
    "            progress.display(idx)\n",
    "\n",
    "    return losses.avg, Acc.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:09:03.926949Z",
     "start_time": "2022-02-19T09:09:03.837488Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def validation(model, val_data, loss_fn):\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    Acc = AverageMeter('Acc', ':6.2f')\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for idx, [x, y, w] in enumerate(val_data):\n",
    "        input = x\n",
    "        target = y.unsqueeze(1).to(device, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(input)\n",
    "            val_loss = loss_fn(predictions, target)\n",
    "        acc = binary_acc(predictions, target)\n",
    "        Acc.update(acc, input.size(0))\n",
    "        losses.update(val_loss.item(), input.size(0))\n",
    "\n",
    "    return losses.avg, Acc.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:09:04.026999Z",
     "start_time": "2022-02-19T09:09:03.933140Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, patience=0, verbose=0):\n",
    "        self._step = 0\n",
    "        self._loss = float('inf')\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def validate(self, loss):\n",
    "        if self._loss < loss:\n",
    "            self._step += 1\n",
    "            if self._step > self.patience:\n",
    "                if self.verbose:\n",
    "                    print(f'\\n Training process is stopped early....')\n",
    "                return True\n",
    "        else:\n",
    "            self._step = 0\n",
    "            self._loss = loss\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:09:47.568667Z",
     "start_time": "2022-02-19T09:09:04.030005Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch :  1   Train Loss: 0.4283  Train Acc: 78.47  Valid Loss: 0.441  Valid Acc: 75.83\n",
      "[Train] Epoch :  2   Train Loss: 0.3779  Train Acc: 80.88  Valid Loss: 0.3644  Valid Acc: 82.16\n",
      "[Train] Epoch :  3   Train Loss: 0.3681  Train Acc: 81.07  Valid Loss: 0.3639  Valid Acc: 81.25\n",
      "[Train] Epoch :  4   Train Loss: 0.3654  Train Acc: 81.43  Valid Loss: 0.357  Valid Acc: 82.07\n",
      "[Train] Epoch :  5   Train Loss: 0.3615  Train Acc: 81.82  Valid Loss: 0.3581  Valid Acc: 81.83\n",
      "[Train] Epoch :  6   Train Loss: 0.3611  Train Acc: 81.76  Valid Loss: 0.3557  Valid Acc: 81.84\n",
      "[Train] Epoch :  7   Train Loss: 0.3583  Train Acc: 81.92  Valid Loss: 0.3579  Valid Acc: 81.36\n",
      "[Train] Epoch :  8   Train Loss: 0.3543  Train Acc: 82.36  Valid Loss: 0.3601  Valid Acc: 82.27\n",
      "[Train] Epoch :  9   Train Loss: 0.3442  Train Acc: 83.11  Valid Loss: 0.3346  Valid Acc: 83.22\n",
      "[Train] Epoch : 10   Train Loss: 0.336  Train Acc: 83.63  Valid Loss: 0.3342  Valid Acc: 83.87\n",
      "[Train] Epoch : 11   Train Loss: 0.3316  Train Acc: 84.09  Valid Loss: 0.3331  Valid Acc: 83.96\n",
      "[Train] Epoch : 12   Train Loss: 0.3299  Train Acc: 84.05  Valid Loss: 0.3331  Valid Acc: 84.51\n",
      "[Train] Epoch : 13   Train Loss: 0.3281  Train Acc: 83.97  Valid Loss: 0.3463  Valid Acc: 82.52\n",
      "[Train] Epoch : 14   Train Loss: 0.3262  Train Acc: 84.32  Valid Loss: 0.3337  Valid Acc: 83.26\n",
      "[Train] Epoch : 15   Train Loss: 0.3244  Train Acc: 84.5  Valid Loss: 0.3314  Valid Acc: 84.24\n",
      "CPU times: user 35 s, sys: 12.5 s, total: 47.5 s\n",
      "Wall time: 43.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best = {\"acc\": sys.float_info.min}\n",
    "history = dict()\n",
    "early_stopping = EarlyStopping(patience=5, verbose=1)\n",
    "\n",
    "for epoch in range(1, params['NUM_EPOCHS']+1):\n",
    "    epoch_loss, epoch_acc = train(baseline_model, train_loader, optimizer, loss_fn, use_fp16=False)\n",
    "    val_loss, val_acc = validation(baseline_model, test_loader, loss_fn)\n",
    "\n",
    "    history.setdefault('loss', []).append(epoch_loss)\n",
    "    history.setdefault('val_loss', []).append(val_loss)\n",
    "    history.setdefault('accuracy', []).append(epoch_acc)\n",
    "    history.setdefault('val_accuracy', []).append(val_acc)\n",
    "\n",
    "    print(f\"[Train] Epoch : {epoch:^3}\"\n",
    "          f\"  Train Loss: {epoch_loss:.4}\"\n",
    "          f\"  Train Acc: {epoch_acc:.4}\"\n",
    "          f\"  Valid Loss: {val_loss:.4}\"\n",
    "          f\"  Valid Acc: {val_acc:.4}\")\n",
    "\n",
    "    if val_acc > best[\"acc\"]:\n",
    "        best[\"state\"] = baseline_model.state_dict()\n",
    "        best[\"acc\"] = val_acc\n",
    "        best[\"epoch\"] = epoch\n",
    "    if early_stopping.validate(val_loss):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T10:23:25.631778Z",
     "start_time": "2022-02-19T10:23:24.984554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.33143243851323995, Acc : 84.24415588378906\n"
     ]
    }
   ],
   "source": [
    "losses = AverageMeter('Loss', ':.4e')\n",
    "Acc = AverageMeter('Acc', ':6.2f')\n",
    "\n",
    "baseline_model.eval()\n",
    "test_loss = 0\n",
    "predic = []\n",
    "for idx, [x, y, w] in enumerate(test_loader):\n",
    "    input = x\n",
    "    target = y.unsqueeze(1).to(device, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        predictions = baseline_model(input)\n",
    "        test_loss = loss_fn(predictions, target)\n",
    "    predic.extend(torch.round(torch.sigmoid(predictions)).detach().cpu().squeeze().tolist())\n",
    "    acc = binary_acc(predictions, target)\n",
    "    Acc.update(acc, input.size(0))\n",
    "    losses.update(test_loss.item(), input.size(0))\n",
    "\n",
    "print(f'Loss : {losses.avg}, Acc : {Acc.avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T10:24:29.715705Z",
     "start_time": "2022-02-19T10:24:29.605555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11798,   637],\n",
       "       [ 1933,  1913]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_data[TARGET_FEATURE_NAME].map(lambda x: TARGET_LABELS.index(x)).tolist(), predic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Experiment 2: TabTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The TabTransformer architecture works as follows:\n",
    "\n",
    "1. All the categorical features are encoded as embeddings, using the same embedding_dims. This means that each value in each categorical feature will have its own embedding vector.\n",
    "2. A column embedding, one embedding vector for each categorical feature, is added (point-wise) to the categorical feature embedding.\n",
    "3. The embedded categorical features are fed into a stack of Transformer blocks. Each Transformer block consists of a multi-head self-attention layer followed by a feed-forward layer.\n",
    "4. The outputs of the final Transformer layer, which are the contextual embeddings of the categorical features, are concatenated with the input numerical features, and fed into a final MLP block.\n",
    "5. A softmax classifer is applied at the end of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src='./images/tabtransformer.png' width='400'> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/tabtransformer_1.png' width='1000'> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference : https://github.com/lucidrains/tab-transformer-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:09:47.585191Z",
     "start_time": "2022-02-19T09:09:47.572198Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# classes\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:09:47.651599Z",
     "start_time": "2022-02-19T09:09:47.587538Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# attention\n",
    "\n",
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gates = x.chunk(2, dim = -1)\n",
    "        return x * F.gelu(gates)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult = 4, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult * 2),\n",
    "            GEGLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            heads = 8,\n",
    "            dim_head = 16,\n",
    "            dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.heads\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
    "        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:09:47.742216Z",
     "start_time": "2022-02-19T09:09:47.655197Z"
    }
   },
   "outputs": [],
   "source": [
    "class create_tabtransformer_classifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 FEATURE_NAMES=FEATURE_NAMES,\n",
    "                 CATEGORICAL_FEATURES_WITH_VOCABULARY=CATEGORICAL_FEATURES_WITH_VOCABULARY,\n",
    "                 USE_COLUMN_EMBEDDING=False):\n",
    "        super(create_tabtransformer_classifier, self).__init__()\n",
    "        \n",
    "        self.num_transformer_blocks = params['NUM_TRANSFORMER_BLOCKS']\n",
    "        self.num_heads = params['NUM_HEADS']\n",
    "        self.embedding_dims = params['EMBEDDING_DIMS']\n",
    "        self.mlp_hidden_units_factors = params['MLP_HIDDEN_UNITS_FACTORS']\n",
    "        self.dropout_rate = params['DROPOUT_RATE']\n",
    "        self.device = params['DEVICE']\n",
    "        \n",
    "        self.num_features = len(FEATURE_NAMES)\n",
    "        self.num_cat_features = len(CATEGORICAL_FEATURES_WITH_VOCABULARY)\n",
    "        self.use_column_embedding = USE_COLUMN_EMBEDDING\n",
    "        self.num_numeric_features = self.num_features - self.num_cat_features\n",
    "        self.embedding_layers = nn.ModuleList()\n",
    "        self.transformer_blocks = nn.ModuleList()\n",
    "\n",
    "        for feature in FEATURE_NAMES:\n",
    "            if feature in CATEGORICAL_FEATURES_WITH_VOCABULARY.keys():\n",
    "                emb = nn.Embedding(len(CATEGORICAL_FEATURES_WITH_VOCABULARY[feature]), self.embedding_dims)\n",
    "                self.embedding_layers.append(emb)\n",
    "\n",
    "        if self.use_column_embedding:\n",
    "            self.column_emb = nn.Embedding(self.num_cat_features, self.embedding_dims)\n",
    "\n",
    "        for block_idx in range(self.num_transformer_blocks):\n",
    "            self.transformer_blocks.append(nn.ModuleList([\n",
    "                Residual(PreNorm(self.embedding_dims, Attention(self.embedding_dims, heads = self.num_heads, dim_head = self.embedding_dims, dropout = self.dropout_rate))),\n",
    "                Residual(PreNorm(self.embedding_dims, FeedForward(self.embedding_dims, dropout = self.dropout_rate))),\n",
    "            ]))\n",
    "\n",
    "        input_size = (self.embedding_dims * self.num_cat_features) + self.num_numeric_features\n",
    "        mlp_hidden_units = [factor * input_size for factor in self.mlp_hidden_units_factors]\n",
    "        mlp_hidden_units.insert(0, input_size)\n",
    "\n",
    "        self.MLP = create_mlp(mlp_hidden_units,\n",
    "                              self.dropout_rate,\n",
    "                              nn.SELU(),\n",
    "                              'BatchNorm')\n",
    "\n",
    "        self.classifier = nn.Linear(mlp_hidden_units[-1], 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        numeric_vectors = []\n",
    "        for i in range(self.num_numeric_features):\n",
    "            # emb = self.linear_layers[i](inputs[:, i].view(inputs.size(0), -1, 1).float().to(self.device))\n",
    "            emb = inputs[:, i].view(inputs.size(0), -1, 1).float().to(self.device)\n",
    "            numeric_vectors.append(emb)\n",
    "        numeric_vectors = torch.cat(numeric_vectors, dim=1)\n",
    "        numeric_vectors = numeric_vectors.squeeze()\n",
    "\n",
    "        categorical_vectors = []\n",
    "        for i in range(self.num_cat_features):\n",
    "            emb = self.embedding_layers[i](inputs[:, self.num_numeric_features+i].view(inputs.size(0), -1).long().to(self.device))\n",
    "            categorical_vectors.append(emb)\n",
    "        categorical_vectors = torch.cat(categorical_vectors, dim=1)\n",
    "\n",
    "        for attn, ff in self.transformer_blocks:\n",
    "            categorical_vectors = attn(categorical_vectors)\n",
    "            categorical_vectors = ff(categorical_vectors)\n",
    "\n",
    "        categorical_vectors = rearrange(categorical_vectors, 'b n d -> b (n d)')\n",
    "        embeddings = torch.cat([numeric_vectors,categorical_vectors], dim=1)\n",
    "\n",
    "        features = embeddings\n",
    "\n",
    "        features = self.MLP(features)\n",
    "        output = self.classifier(features)\n",
    "\n",
    "        return output.type(torch.FloatTensor).to(self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:09:47.847973Z",
     "start_time": "2022-02-19T09:09:47.745720Z"
    }
   },
   "outputs": [],
   "source": [
    "tabtransformer = create_tabtransformer_classifier(params).to(device)\n",
    "optimizer = optim.Adam(tabtransformer.parameters(), lr=params['LEARNING_RATE'])\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:09:47.953004Z",
     "start_time": "2022-02-19T09:09:47.850768Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "create_tabtransformer_classifier                   --                        --\n",
       "├─ModuleList: 1-1                                  --                        --\n",
       "├─ModuleList: 1-2                                  --                        --\n",
       "│    └─ModuleList: 2-1                             --                        --\n",
       "│    └─ModuleList: 2-2                             --                        --\n",
       "│    └─ModuleList: 2-3                             --                        --\n",
       "├─ModuleList: 1-1                                  --                        --\n",
       "│    └─Embedding: 2-4                              [265, 1, 16]              144\n",
       "│    └─Embedding: 2-5                              [265, 1, 16]              256\n",
       "│    └─Embedding: 2-6                              [265, 1, 16]              112\n",
       "│    └─Embedding: 2-7                              [265, 1, 16]              240\n",
       "│    └─Embedding: 2-8                              [265, 1, 16]              96\n",
       "│    └─Embedding: 2-9                              [265, 1, 16]              80\n",
       "│    └─Embedding: 2-10                             [265, 1, 16]              32\n",
       "│    └─Embedding: 2-11                             [265, 1, 16]              672\n",
       "├─ModuleList: 1-2                                  --                        --\n",
       "│    └─ModuleList: 2-1                             --                        --\n",
       "│    │    └─Residual: 3-1                          [265, 8, 16]              4,144\n",
       "│    │    └─Residual: 3-2                          [265, 8, 16]              3,248\n",
       "│    └─ModuleList: 2-2                             --                        --\n",
       "│    │    └─Residual: 3-3                          [265, 8, 16]              4,144\n",
       "│    │    └─Residual: 3-4                          [265, 8, 16]              3,248\n",
       "│    └─ModuleList: 2-3                             --                        --\n",
       "│    │    └─Residual: 3-5                          [265, 8, 16]              4,144\n",
       "│    │    └─Residual: 3-6                          [265, 8, 16]              3,248\n",
       "├─create_mlp: 1-3                                  [265, 133]                --\n",
       "│    └─Sequential: 2-12                            [265, 133]                --\n",
       "│    │    └─Sequential: 3-7                        [265, 266]                35,910\n",
       "│    │    └─Sequential: 3-8                        [265, 133]                36,043\n",
       "├─Linear: 1-4                                      [265, 1]                  134\n",
       "====================================================================================================\n",
       "Total params: 95,895\n",
       "Trainable params: 95,895\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 25.41\n",
       "====================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 21.50\n",
       "Params size (MB): 0.38\n",
       "Estimated Total Size (MB): 21.90\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(tabtransformer, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:10:54.080899Z",
     "start_time": "2022-02-19T09:09:47.956576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch :  1   Train Loss: 0.4034  Train Acc: 80.9  Valid Loss: 0.3334  Valid Acc: 84.64\n",
      "[Train] Epoch :  2   Train Loss: 0.3411  Train Acc: 84.07  Valid Loss: 0.3298  Valid Acc: 84.34\n",
      "[Train] Epoch :  3   Train Loss: 0.3351  Train Acc: 84.36  Valid Loss: 0.3233  Valid Acc: 84.8\n",
      "[Train] Epoch :  4   Train Loss: 0.3299  Train Acc: 84.58  Valid Loss: 0.3233  Valid Acc: 84.78\n",
      "[Train] Epoch :  5   Train Loss: 0.3291  Train Acc: 84.58  Valid Loss: 0.3204  Valid Acc: 85.04\n",
      "[Train] Epoch :  6   Train Loss: 0.3261  Train Acc: 84.65  Valid Loss: 0.3205  Valid Acc: 84.97\n",
      "[Train] Epoch :  7   Train Loss: 0.3256  Train Acc: 84.69  Valid Loss: 0.3175  Valid Acc: 85.22\n",
      "[Train] Epoch :  8   Train Loss: 0.3237  Train Acc: 84.68  Valid Loss: 0.3213  Valid Acc: 84.87\n",
      "[Train] Epoch :  9   Train Loss: 0.3241  Train Acc: 84.9  Valid Loss: 0.3179  Valid Acc: 85.16\n",
      "[Train] Epoch : 10   Train Loss: 0.3193  Train Acc: 84.94  Valid Loss: 0.3218  Valid Acc: 84.66\n",
      "[Train] Epoch : 11   Train Loss: 0.3204  Train Acc: 85.14  Valid Loss: 0.3136  Valid Acc: 85.45\n",
      "[Train] Epoch : 12   Train Loss: 0.3181  Train Acc: 85.26  Valid Loss: 0.3162  Valid Acc: 85.1\n",
      "[Train] Epoch : 13   Train Loss: 0.3178  Train Acc: 85.1  Valid Loss: 0.3138  Valid Acc: 85.49\n",
      "[Train] Epoch : 14   Train Loss: 0.3174  Train Acc: 85.16  Valid Loss: 0.315  Valid Acc: 85.46\n",
      "[Train] Epoch : 15   Train Loss: 0.3153  Train Acc: 85.32  Valid Loss: 0.3158  Valid Acc: 85.18\n",
      "CPU times: user 59.5 s, sys: 11 s, total: 1min 10s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best = {\"acc\": sys.float_info.min}\n",
    "history = dict()\n",
    "early_stopping = EarlyStopping(patience=5, verbose=1)\n",
    "\n",
    "for epoch in range(1, params['NUM_EPOCHS']+1):\n",
    "    epoch_loss, epoch_acc = train(tabtransformer, train_loader, optimizer, loss_fn, use_fp16=False)\n",
    "    val_loss, val_acc = validation(tabtransformer, test_loader, loss_fn)\n",
    "\n",
    "    history.setdefault('loss', []).append(epoch_loss)\n",
    "    history.setdefault('val_loss', []).append(val_loss)\n",
    "    history.setdefault('accuracy', []).append(epoch_acc)\n",
    "    history.setdefault('val_accuracy', []).append(val_acc)\n",
    "\n",
    "    print(f\"[Train] Epoch : {epoch:^3}\"\n",
    "          f\"  Train Loss: {epoch_loss:.4}\"\n",
    "          f\"  Train Acc: {epoch_acc:.4}\"\n",
    "          f\"  Valid Loss: {val_loss:.4}\"\n",
    "          f\"  Valid Acc: {val_acc:.4}\")\n",
    "\n",
    "    if val_acc > best[\"acc\"]:\n",
    "        best[\"state\"] = tabtransformer.state_dict()\n",
    "        best[\"acc\"] = val_acc\n",
    "        best[\"epoch\"] = epoch\n",
    "    if early_stopping.validate(val_loss):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T10:25:01.650697Z",
     "start_time": "2022-02-19T10:25:00.852162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.31577428002485913, Acc : 85.18414306640625\n"
     ]
    }
   ],
   "source": [
    "losses = AverageMeter('Loss', ':.4e')\n",
    "Acc = AverageMeter('Acc', ':6.2f')\n",
    "\n",
    "tabtransformer.eval()\n",
    "test_loss = 0\n",
    "predic = []\n",
    "for idx, [x, y, w] in enumerate(test_loader):\n",
    "    input = x\n",
    "    target = y.unsqueeze(1).to(device, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        predictions = tabtransformer(input)\n",
    "        test_loss = loss_fn(predictions, target)\n",
    "    predic.extend(torch.round(torch.sigmoid(predictions)).detach().cpu().squeeze().tolist())\n",
    "    acc = binary_acc(predictions, target)\n",
    "    Acc.update(acc, input.size(0))\n",
    "    losses.update(test_loss.item(), input.size(0))\n",
    "\n",
    "print(f'Loss : {losses.avg}, Acc : {Acc.avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T10:25:15.523575Z",
     "start_time": "2022-02-19T10:25:15.412123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11452,   983],\n",
       "       [ 1434,  2412]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_data[TARGET_FEATURE_NAME].map(lambda x: TARGET_LABELS.index(x)).tolist(), predic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with official Tabtransformer code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:10:54.089165Z",
     "start_time": "2022-02-19T09:10:54.083951Z"
    }
   },
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:10:54.172989Z",
     "start_time": "2022-02-19T09:10:54.091847Z"
    }
   },
   "outputs": [],
   "source": [
    "# transformer\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_tokens, dim, depth, heads, dim_head, attn_dropout, ff_dropout):\n",
    "        super().__init__()\n",
    "        self.embeds = nn.Embedding(num_tokens, dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Residual(PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = attn_dropout))),\n",
    "                Residual(PreNorm(dim, FeedForward(dim, dropout = ff_dropout))),\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeds(x)\n",
    "\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x)\n",
    "            x = ff(x)\n",
    "\n",
    "        return x\n",
    "# mlp\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dims, act = None):\n",
    "        super().__init__()\n",
    "        dims_pairs = list(zip(dims[:-1], dims[1:]))\n",
    "        layers = []\n",
    "        for ind, (dim_in, dim_out) in enumerate(dims_pairs):\n",
    "            is_last = ind >= (len(dims_pairs) - 1)\n",
    "            linear = nn.Linear(dim_in, dim_out)\n",
    "            layers.append(linear)\n",
    "\n",
    "            if is_last:\n",
    "                continue\n",
    "\n",
    "            act = default(act, nn.ReLU())\n",
    "            layers.append(act)\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T09:10:54.284687Z",
     "start_time": "2022-02-19T09:10:54.176266Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# main class\n",
    "\n",
    "class TabTransformer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *,\n",
    "            categories,\n",
    "            num_continuous,\n",
    "            dim,\n",
    "            depth,\n",
    "            heads,\n",
    "            dim_head = 16,\n",
    "            dim_out = 1,\n",
    "            mlp_hidden_mults = (4, 2),\n",
    "            mlp_act = None,\n",
    "            num_special_tokens = 2,\n",
    "            continuous_mean_std = None,\n",
    "            attn_dropout = 0.,\n",
    "            ff_dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert all(map(lambda n: n > 0, categories)), 'number of each category must be positive'\n",
    "\n",
    "        # categories related calculations\n",
    "\n",
    "        self.num_categories = len(categories)\n",
    "        self.num_unique_categories = sum(categories)\n",
    "\n",
    "        # create category embeddings table\n",
    "\n",
    "        self.num_special_tokens = num_special_tokens\n",
    "        total_tokens = self.num_unique_categories + num_special_tokens\n",
    "\n",
    "        # for automatically offsetting unique category ids to the correct position in the categories embedding table\n",
    "\n",
    "        categories_offset = F.pad(torch.tensor(list(categories)), (1, 0), value = num_special_tokens)\n",
    "        categories_offset = categories_offset.cumsum(dim = -1)[:-1]\n",
    "        self.register_buffer('categories_offset', categories_offset)\n",
    "\n",
    "        # continuous\n",
    "\n",
    "        if exists(continuous_mean_std):\n",
    "            assert continuous_mean_std.shape == (num_continuous, 2), f'continuous_mean_std must have a shape of ({num_continuous}, 2) where the last dimension contains the mean and variance respectively'\n",
    "        self.register_buffer('continuous_mean_std', continuous_mean_std)\n",
    "\n",
    "        self.norm = nn.LayerNorm(num_continuous)\n",
    "        self.num_continuous = num_continuous\n",
    "\n",
    "        # transformer\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            num_tokens = total_tokens,\n",
    "            dim = dim,\n",
    "            depth = depth,\n",
    "            heads = heads,\n",
    "            dim_head = dim_head,\n",
    "            attn_dropout = attn_dropout,\n",
    "            ff_dropout = ff_dropout\n",
    "        )\n",
    "\n",
    "        # mlp to logits\n",
    "\n",
    "        input_size = (dim * self.num_categories) + num_continuous\n",
    "        l = input_size // 8\n",
    "\n",
    "        hidden_dimensions = list(map(lambda t: l * t, mlp_hidden_mults))\n",
    "        all_dimensions = [input_size, *hidden_dimensions, dim_out]\n",
    "\n",
    "        self.mlp = MLP(all_dimensions, act = mlp_act)\n",
    "\n",
    "    def forward(self, x_categ, x_cont):\n",
    "        assert x_categ.shape[-1] == self.num_categories, f'you must pass in {self.num_categories} values for your categories input'\n",
    "        x_categ += self.categories_offset\n",
    "\n",
    "        x = self.transformer(x_categ)\n",
    "\n",
    "        flat_categ = x.flatten(1)\n",
    "\n",
    "        assert x_cont.shape[1] == self.num_continuous, f'you must pass in {self.num_continuous} values for your continuous input'\n",
    "\n",
    "        if exists(self.continuous_mean_std):\n",
    "            mean, std = self.continuous_mean_std.unbind(dim = -1)\n",
    "            x_cont = (x_cont - mean) / std\n",
    "\n",
    "        normed_cont = self.norm(x_cont)\n",
    "\n",
    "        x = torch.cat((flat_categ, normed_cont), dim = -1)\n",
    "        return self.mlp(x)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "ko",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "ko",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
