{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T10:32:44.606606Z",
     "start_time": "2022-07-11T10:32:43.194579Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import flow_ssl\n",
    "from experiments.train_flows import utils\n",
    "\n",
    "import torch\n",
    "from torch import distributions\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.utils import _pair, _quadruple\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import SVHN\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import wandb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dataloader import get_transform, get_loader\n",
    "from utils import seed_everything, get_percentile, schedule, MedianPool2d, AverageMeter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argument Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T10:32:44.616714Z",
     "start_time": "2022-07-11T10:32:44.608500Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_args(notebook=False, print_=False):\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--dataset', type=str, default=\"cifar10\", metavar='DATA',\n",
    "                        help='Dataset name (lower case, default: cifar0),\\\n",
    "                        opt : [cifar10, svhn, mnist, fmnist]')\n",
    "    parser.add_argument('--data_path', type=str, default=None, metavar='PATH',\n",
    "                        help='path to datasets location (default: None)')\n",
    "\n",
    "    parser.add_argument('--ood_dataset', type=str, default=\"svhn\", metavar='DATA',\n",
    "                        help='OOD dataset name (lower case, default: svhn),\\\n",
    "                        opt : [cifar10, svhn, mnist, fmnist]')\n",
    "    parser.add_argument('--ood_data_path', type=str, default=None, metavar='PATH',\n",
    "                        help='path to ood datasets location (default: None)')\n",
    "    \n",
    "    parser.add_argument('--logdir', type=str, default=None, metavar='PATH',\n",
    "                        help='path to log directory (default: None)')\n",
    "    parser.add_argument('--ckptdir', type=str, default=None, metavar='PATH',\n",
    "                        help='path to ckpt directory (default: None)')\n",
    "\n",
    "    parser.add_argument('--batch_size', default=32, type=int, help='Batch size')\n",
    "    parser.add_argument('--lr', default=1e-3, type=float, help='Learning rate')\n",
    "    parser.add_argument('--max_grad_norm', type=float, default=100., help='Max gradient norm for clipping')\n",
    "    parser.add_argument('--num_epochs', default=101, type=int, help='Number of epochs to train')\n",
    "    parser.add_argument('--num_samples', default=20, type=int, help='Number of samples at test time')\n",
    "    parser.add_argument('--num_workers', default=8, type=int, help='Number of data loader threads')\n",
    "    parser.add_argument('--resume',  type=str, default=None, metavar='PATH', help='path to ckpt')\n",
    "    parser.add_argument('--weight_decay', default=5e-5, type=float,\n",
    "                        help='L2 regularization (only applied to the weight norm scale factors)')\n",
    "\n",
    "    parser.add_argument('--save_freq', default=25, type=int, help='frequency of saving ckpts')\n",
    "    parser.add_argument('--negative_val', default=-100_000, type=int, help='Negative loss threshold')\n",
    "    \n",
    "    parser.add_argument('--flow', type=str, default=\"RealNVP\", help=\"Flow model to use (default: RealNVP) \\\n",
    "                        choices=['RealNVP', 'Glow', 'RealNVPNewMask', 'RealNVPNewMask2', 'RealNVPSmall']\")\n",
    "    parser.add_argument('--num_blocks', default=8, type=int, help='number of blocks in ResNet')\n",
    "    parser.add_argument('--num_scales', default=3, type=int, help='number of scales in multi-layer architecture')\n",
    "    parser.add_argument('--num_mid_channels', default=64, type=int, help='number of channels \\\n",
    "                                                                          in coupling layer parametrizing network')\n",
    "    parser.add_argument('--no_batchnorm', action='store_true')\n",
    "    parser.add_argument('--st_type', choices=['highway', 'resnet', 'convnet'], default='resnet')\n",
    "    parser.add_argument('--aug', action='store_true')\n",
    "    parser.add_argument('--init_zeros', action='store_true')\n",
    "    parser.add_argument('--optim', choices=['Adam', 'RMSprop'], default='Adam')\n",
    "    parser.add_argument('--lr_anneal', action='store_true')\n",
    "\n",
    "    args = parser.parse_args([]) if notebook else parser.parse_args()\n",
    "    \n",
    "    if print_:\n",
    "        parser.print_help()\n",
    "        \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T10:32:44.622608Z",
     "start_time": "2022-07-11T10:32:44.617897Z"
    }
   },
   "outputs": [],
   "source": [
    "args = parse_args(True, print_=False)\n",
    "\n",
    "args.data_path = 'experiments/datasets'\n",
    "args.ood_data_path = 'experiments/datasets'\n",
    "args.ckptdir = 'ckpt'\n",
    "args.logdir = 'log'\n",
    "args.lr_anneal = True\n",
    "args.lr = 5e-5\n",
    "args.save_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T10:32:44.651650Z",
     "start_time": "2022-07-11T10:32:44.623549Z"
    }
   },
   "outputs": [],
   "source": [
    "case = f'IN-{args.dataset}_OOD-{args.ood_dataset}_epochs-{args.num_epochs}_flow-{args.flow}_lr-{args.lr}_anneal-{args.lr_anneal}_\\nblocks-{args.num_blocks}_nscales-{args.num_scales}_st-{args.st_type}_aug-{args.aug}_optim-{args.optim}'\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T10:32:44.655249Z",
     "start_time": "2022-07-11T10:32:44.652659Z"
    }
   },
   "outputs": [],
   "source": [
    "use_wandb = False\n",
    "if use_wandb:\n",
    "    wandb.init(project=\"NF_neg_training\", entity=\"jskim0406\", name=f'{case}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T10:32:44.660082Z",
     "start_time": "2022-07-11T10:32:44.656169Z"
    }
   },
   "outputs": [],
   "source": [
    "seed_everything(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T10:33:05.542459Z",
     "start_time": "2022-07-11T10:33:05.536140Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform_train_c10, transform_test_c10, img_shape = get_transform(args, 'cifar10')\n",
    "transform_train_svhn, transform_test_svhn, _ = get_transform(args, 'svhn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T10:33:09.937996Z",
     "start_time": "2022-07-11T10:33:08.691848Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainloader, testloader, _ = get_loader(args, 'cifar10', transform_train_c10, transform_test_c10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T10:33:12.410573Z",
     "start_time": "2022-07-11T10:33:09.952700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: experiments/datasets/train_32x32.mat\n",
      "Using downloaded and verified file: experiments/datasets/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "ood_trainloader, ood_testloader, _ = get_loader(args, 'svhn', transform_train_svhn, transform_test_svhn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T10:33:13.387281Z",
     "start_time": "2022-07-11T10:33:13.381397Z"
    }
   },
   "outputs": [],
   "source": [
    "model_cfg = getattr(flow_ssl, args.flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T10:33:14.302349Z",
     "start_time": "2022-07-11T10:33:13.538694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model contains 87,859,080 parameters\n"
     ]
    }
   ],
   "source": [
    "if 'RealNVP' in args.flow:\n",
    "    net = model_cfg(in_channels=img_shape[0], \n",
    "                    init_zeros=args.init_zeros, \n",
    "                    mid_channels=args.num_mid_channels,\n",
    "                    num_scales=args.num_scales, \n",
    "                    st_type=args.st_type, \n",
    "                    use_batch_norm=not args.no_batchnorm)\n",
    "    \n",
    "elif args.flow == 'Glow':\n",
    "    net = model_cfg(image_shape=img_shape, \n",
    "                    mid_channels=args.num_mid_channels, \n",
    "                    num_scales=args.num_scales,\n",
    "                    num_coupling_layers_per_scale=args.num_coupling_layers_per_scale, \n",
    "                    num_layers=args.num_blocks,\n",
    "                    multi_scale=not args.no_multi_scale, \n",
    "                    st_type=args.st_type)\n",
    "\n",
    "print(f'Model contains {format(sum([p.numel() for p in net.parameters()]), \",d\")} parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T10:33:16.790477Z",
     "start_time": "2022-07-11T10:33:14.361364Z"
    }
   },
   "outputs": [],
   "source": [
    "D = int(np.prod(img_shape))\n",
    "\n",
    "prior = distributions.MultivariateNormal(torch.zeros(D).to(device), torch.eye(D).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T10:33:16.854586Z",
     "start_time": "2022-07-11T10:33:16.849944Z"
    }
   },
   "outputs": [],
   "source": [
    "class FlowLoss(nn.Module):\n",
    "    \"\"\"Get the NLL loss for a RealNVP model.\n",
    "\n",
    "    Args:\n",
    "        k (int or float): Number of discrete values in each input dimension.\n",
    "            E.g., `k` is 256 for natural images.\n",
    "\n",
    "    See Also:\n",
    "        Equation (3) in the RealNVP paper: https://arxiv.org/abs/1605.08803\n",
    "    \"\"\"\n",
    "    # Get 'Bits Per Dimension(BPD)' by subtracting \"np.log(self.k) * np.prod(z.size()[1:])\"\n",
    "    # ref : https://github.com/openai/glow/issues/43\n",
    "\n",
    "    def __init__(self, prior, k=256):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.prior = prior\n",
    "\n",
    "    def forward(self, z, sldj, y=None, mean=True):\n",
    "        z = z.reshape((z.shape[0], -1))\n",
    "        # prior_ll : negative value(log prob), -inf에서 0으로 가까워질 수록 data likelihood가 높아지는 것을 의미함\n",
    "        if y is not None:\n",
    "            prior_ll = self.prior.log_prob(z, y)\n",
    "        else:\n",
    "            prior_ll = self.prior.log_prob(z)\n",
    "            \n",
    "        corrected_prior_ll = prior_ll - np.log(self.k) * np.prod(z.size()[1:]) \n",
    "\n",
    "        ll = corrected_prior_ll + sldj\n",
    "        nll = -ll.mean() if mean else -ll  # nll : positive value(), inf에서 0으로 가까워 질수록 data likelihood가 높아지는 것을 의미함\n",
    "\n",
    "        return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T10:33:16.909048Z",
     "start_time": "2022-07-11T10:33:16.907421Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = FlowLoss(prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T10:33:16.963387Z",
     "start_time": "2022-07-11T10:33:16.955570Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'RealNVP' in args.flow:\n",
    "    # We need this to make sure that weight decay is only applied to g -- norm parameter in Weight Normalization\n",
    "    # RealNVP paper: https://arxiv.org/abs/1605.08803 \"3.7 Batch Normalization\"\n",
    "    # RealNVP use ResNet with Batch Normalization and Weight Normalization\n",
    "    param_groups = utils.get_param_groups(net, args.weight_decay, norm_suffix='weight_g')\n",
    "    if args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(param_groups, lr=args.lr)\n",
    "    else:\n",
    "        optimizer = optim.RMSprop(param_groups, lr=args.lr)\n",
    "\n",
    "elif args.flow == 'Glow':\n",
    "    if args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    else:\n",
    "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T10:33:26.410346Z",
     "start_time": "2022-07-11T10:33:26.407965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(param_groups) : 2\n",
      "param_groups[0].keys : dict_keys(['name', 'params', 'weight_decay', 'lr', 'betas', 'eps', 'amsgrad'])\n",
      "param_groups[1].keys : dict_keys(['name', 'params', 'lr', 'betas', 'eps', 'weight_decay', 'amsgrad'])\n"
     ]
    }
   ],
   "source": [
    "print(f'len(param_groups) : {len(param_groups)}')\n",
    "print(f'param_groups[0].keys : {param_groups[0].keys()}')\n",
    "print(f'param_groups[1].keys : {param_groups[1].keys()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T10:33:35.874177Z",
     "start_time": "2022-07-11T10:33:35.866866Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch, net, trainloader, ood_loader, device, optimizer, loss_fn, \n",
    "         max_grad_norm, negative_val=-1e5, num_samples=10, log_freq=100):\n",
    "\n",
    "    print(f'\\nEPOCH : {epoch}')\n",
    "    \n",
    "    net.train()\n",
    "    loss_meter, loss_positive_meter, loss_negative_meter = utils.AverageMeter(), utils.AverageMeter(), utils.AverageMeter()\n",
    "    pooler = MedianPool2d(7, padding=3)\n",
    "    \n",
    "    iter_count, batch_count = 0, 0\n",
    "    with tqdm(total=len(trainloader.dataset)) as progress_bar:\n",
    "        for (x, _), (x_transposed, _) in zip(trainloader, ood_loader):\n",
    "\n",
    "            bs = x.shape[0]\n",
    "            iter_count+=1\n",
    "            batch_count+=bs\n",
    "            \n",
    "            x = torch.cat((x, x_transposed), dim=0)\n",
    "            \n",
    "            x = x.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            z = net(x)\n",
    "            sldj = net.logdet()\n",
    "            \n",
    "            # NLL, positive value, inf -> 0으로 갈 수록 density estim better\n",
    "            loss = loss_fn(z, sldj=sldj, mean=False)\n",
    "            \n",
    "            loss[bs:] *= (-1)\n",
    "            loss_positive = loss[:bs]  # NLL Loss for IN (positive value), inf -> 0으로 갈수록 better(LL maximization)\n",
    "            loss_negative = loss[bs:]  # -NLL Loss for OoD (negative value), -inf로 갈수록 better(LL minimization)\n",
    "            \n",
    "            # Indicator function (eq. 7)\n",
    "            if (loss_negative > negative_val).sum() > 0:   # threshold(negative_val, -100_000)보다 작은 Loss인 경우\n",
    "                loss_negative = loss_negative[loss_negative > negative_val]\n",
    "                loss_negative = loss_negative.mean()\n",
    "                loss_positive = loss_positive.mean()\n",
    "                loss = 0.5*(loss_positive + loss_negative)\n",
    "            else:\n",
    "                loss_negative = torch.tensor(0.)   # OoD NLL이 극단적으로 작아지는 경우(exploding), 0으로 대체\n",
    "                loss_positive = loss_positive.mean()\n",
    "                loss = loss_positive\n",
    "                \n",
    "            loss.backward()\n",
    "            utils.clip_grad_norm(optimizer, max_grad_norm)\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            # Log\n",
    "            loss_meter.update(loss.item(), bs)\n",
    "            loss_positive_meter.update(loss_positive.item(), bs)\n",
    "            loss_negative_meter.update(loss_negative.item(), bs)\n",
    "            progress_bar.set_postfix(\n",
    "                pos_bpd=utils.bits_per_dim(x[:bs], loss_positive_meter.avg),\n",
    "                neg_bpd=utils.bits_per_dim(x[bs:], -loss_negative_meter.avg),\n",
    "                neg_loss=loss_negative.mean().item())\n",
    "            progress_bar.update(bs)\n",
    "\n",
    "            if iter_count % log_freq == 0 or batch_count == len(trainloader.dataset):\n",
    "                if use_wandb:\n",
    "                    wandb.log({'epoch' : epoch,\n",
    "                               'train | loss' : loss_meter.avg, \n",
    "                               'train | loss_Pos' : loss_positive_meter.avg,\n",
    "                               'train | loss_Neg' : loss_negative_meter.avg,\n",
    "                               'train | bpd_Pos' : utils.bits_per_dim(x[:bs], loss_positive_meter.avg),\n",
    "                               'train | bpd_Neg' : utils.bits_per_dim(x[bs:], -loss_negative_meter.avg)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T10:33:45.375321Z",
     "start_time": "2022-07-11T10:33:45.368802Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(epoch, net, testloader, device, loss_fn, mode='in'):\n",
    "    net.eval()\n",
    "    loss_meter = utils.AverageMeter()\n",
    "    loss_list = []\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(testloader.dataset)) as progress_bar:\n",
    "            for x, _ in testloader:\n",
    "                x = x.to(device)\n",
    "                z = net(x)\n",
    "                sldj = net.logdet()\n",
    "                losses = loss_fn(z, sldj=sldj, mean=False)  \n",
    "                loss_list.extend([loss.item() for loss in losses])\n",
    "                \n",
    "                loss = losses.mean()   # loss =: NLL (positive value)\n",
    "                loss_meter.update(loss.item(), x.size(0))\n",
    "                \n",
    "                progress_bar.set_postfix(loss=loss_meter.avg,\n",
    "                                         bpd=utils.bits_per_dim(x, loss_meter.avg))\n",
    "                progress_bar.update(x.size(0))\n",
    "\n",
    "    likelihoods = -torch.from_numpy(np.array(loss_list)).float()  # -NLL (negative value)\n",
    "    \n",
    "    if use_wandb:\n",
    "        wandb.log({'epoch' : epoch,\n",
    "                   f'test|loss_{mode}' : loss_meter.avg,\n",
    "                   f'test|bpd_{mode}' : utils.bits_per_dim(x, loss_meter.avg),\n",
    "                   f'test|likelihoods_{mode}' : likelihoods})\n",
    "    \n",
    "    return likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training / test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T10:33:55.879248Z",
     "start_time": "2022-07-11T10:33:54.880091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.75 GiB total capacity; 2.58 GiB already allocated; 13.12 MiB free; 2.62 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-06cdad9203ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     train(epoch, net, trainloader, ood_trainloader, device, optimizer, loss_fn, \n\u001b[0m\u001b[1;32m     12\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m           negative_val=args.negative_val)\n",
      "\u001b[0;32m<ipython-input-18-934f804154cd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, net, trainloader, ood_loader, device, optimizer, loss_fn, max_grad_norm, negative_val, num_samples, log_freq)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0msldj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogdet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/desktop/0.study/AD/flows_ood_share/flow_ssl/realnvp/realnvp.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlogdet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/desktop/0.study/AD/flows_ood_share/flow_ssl/invertible/parts.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mst_off\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/desktop/0.study/AD/flows_ood_share/flow_ssl/realnvp/coupling_layer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, sldj, reverse)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msldj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_change\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_st\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_st_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m#positional encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/desktop/0.study/AD/flows_ood_share/flow_ssl/realnvp/coupling_layer.py\u001b[0m in \u001b[0;36m_get_st\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mx_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_change\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0;31m#st = self.st_net(F.dropout(x_id, training=self.training, p=0.5))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;31m#st = self.st_net(F.dropout(x_id, training=True, p=0.9))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/desktop/0.study/AD/flows_ood_share/flow_ssl/resnet_realnvp/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskips\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mx_skip\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/desktop/0.study/AD/flows_ood_share/flow_ssl/resnet_realnvp/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_batch_norm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/desktop/0.study/AD/flows_ood_share/flow_ssl/resnet_realnvp/resnet_util.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             for hook in itertools.chain(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 439\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.75 GiB total capacity; 2.58 GiB already allocated; 13.12 MiB free; 2.62 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "net.to(device)\n",
    "seed_everything(0)\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + args.num_epochs + 1):\n",
    "    \n",
    "    if args.lr_anneal:\n",
    "        lr = schedule(args, epoch)\n",
    "        utils.adjust_learning_rate(optimizer, lr)\n",
    "        \n",
    "    train(epoch, net, trainloader, ood_trainloader, device, optimizer, loss_fn, \n",
    "          args.max_grad_norm, num_samples=args.num_samples, \n",
    "          negative_val=args.negative_val)\n",
    "    \n",
    "    test_ll = test(epoch, net, testloader, device, loss_fn, mode='in')  # LL (Not NLL)\n",
    "    test_ll_percentile = get_percentile(test_ll)  # 하위 5%의 loss 값 추출(높은 OOD에 해당하는 loss 값)\n",
    "    test_ll = test_ll.cpu().detach().numpy()\n",
    "\n",
    "    if args.ood_dataset:\n",
    "        ood_ll = test(epoch, net, ood_testloader, device, loss_fn, mode='ood')  # LL (Not NLL)\n",
    "        ood_ll_percentile = get_percentile(ood_ll)\n",
    "        ood_ll = ood_ll.cpu().detach().numpy()\n",
    "        \n",
    "        # AUC-ROC\n",
    "        n_ood, n_test = len(ood_ll), len(test_ll)\n",
    "        lls = np.hstack([ood_ll, test_ll])\n",
    "        targets = np.ones((n_ood + n_test,), dtype=int)\n",
    "        targets[:n_ood] = 0\n",
    "        score = roc_auc_score(targets, lls)\n",
    "        if use_wandb:\n",
    "            wandb.log({'ood/roc_auc': score})\n",
    "        \n",
    "\n",
    "    # plotting likelihood hists\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    sns.distplot(test_ll[test_ll > test_ll_percentile], label='test')  # 너무 예외적으로 낮게 뽑힌 Loss는 제외\n",
    "    if args.ood_dataset:\n",
    "        sns.distplot(ood_ll[ood_ll > ood_ll_percentile], label='OOD')\n",
    "    plt.legend()\n",
    "    fig.canvas.draw()\n",
    "    hist_img = torch.tensor(np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=''))\n",
    "    hist_img = torch.tensor(hist_img.reshape(fig.canvas.get_width_height()[::-1] + (3,))).transpose(0, 2).transpose(1, 2)\n",
    "    os.makedirs(os.path.join(args.ckptdir,f'LL_histogram'), exist_ok=True)\n",
    "    plt.savefig(os.path.join(args.ckptdir,f'LL_histogram/{epoch}.png'))\n",
    "    if use_wandb:\n",
    "        wandb.log({f\"LL_histogram\" : [wandb.Image(os.path.join(args.ckptdir,f'LL_histogram/{epoch}.png'))]})\n",
    "\n",
    "    # Save checkpoint\n",
    "    if (epoch % args.save_freq == 0):\n",
    "        print('Saving...')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'lls': lls,\n",
    "            'targets' : targets,\n",
    "            'ood/roc_auc' : score,\n",
    "        }\n",
    "        os.makedirs(args.ckptdir, exist_ok=True)\n",
    "        torch.save(state, os.path.join(args.ckptdir, str(epoch)+'.pt'))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomcrop\n",
    "- [Reference - torchvision official doc](https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T10:33:55.883584Z",
     "start_time": "2022-07-11T10:33:19.290Z"
    }
   },
   "outputs": [],
   "source": [
    "# sphinx_gallery_thumbnail_path = \"../../gallery/assets/transforms_thumbnail.png\"\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "orig_img = Image.open(Path('assets') / 'astronaut.jpg')\n",
    "# if you change the seed, make sure that the randomly-applied transforms\n",
    "# properly show that the image can be both transformed and *not* transformed!\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def plot(imgs, with_orig=True, row_title=None, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0]) + with_orig\n",
    "    fig, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        row = [orig_img] + row if with_orig else row\n",
    "        for col_idx, img in enumerate(row):\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    if with_orig:\n",
    "        axs[0, 0].set(title='Original image')\n",
    "        axs[0, 0].title.set_size(8)\n",
    "    if row_title is not None:\n",
    "        for row_idx in range(num_rows):\n",
    "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T10:33:55.887225Z",
     "start_time": "2022-07-11T10:33:19.435Z"
    }
   },
   "outputs": [],
   "source": [
    "print(orig_img.size)\n",
    "orig_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T10:33:55.890995Z",
     "start_time": "2022-07-11T10:33:19.571Z"
    }
   },
   "outputs": [],
   "source": [
    "cropper = T.RandomCrop(size=(256, 256), padding=4)\n",
    "crops = [cropper(orig_img) for _ in range(4)]\n",
    "plot(crops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T10:33:55.894824Z",
     "start_time": "2022-07-11T10:33:19.703Z"
    }
   },
   "outputs": [],
   "source": [
    "cropper = T.RandomCrop(size=(256, 256), padding=200)\n",
    "crops = [cropper(orig_img) for _ in range(4)]\n",
    "plot(crops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T10:33:55.898433Z",
     "start_time": "2022-07-11T10:33:19.846Z"
    }
   },
   "outputs": [],
   "source": [
    "cropper = T.RandomCrop(size=(256, 256), padding=100)\n",
    "crops = [cropper(orig_img) for _ in range(4)]\n",
    "plot(crops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T10:33:55.901508Z",
     "start_time": "2022-07-11T10:33:19.986Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cropper = T.RandomCrop(size=(128, 128), padding=4)\n",
    "crops = [cropper(orig_img) for _ in range(4)]\n",
    "plot(crops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T10:33:55.905028Z",
     "start_time": "2022-07-11T10:33:20.122Z"
    }
   },
   "outputs": [],
   "source": [
    "cropper = T.RandomCrop(size=(128, 128))\n",
    "crops = [cropper(orig_img) for _ in range(4)]\n",
    "plot(crops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T10:33:55.908801Z",
     "start_time": "2022-07-11T10:33:20.394Z"
    }
   },
   "outputs": [],
   "source": [
    "prior = distributions.MultivariateNormal(torch.zeros(2).to(device),\n",
    "                                         torch.eye(2).to(device))\n",
    "\n",
    "prior_examples = prior.sample(sample_shape=[5_000])\n",
    "exmp_df = pd.DataFrame({'x':prior_examples[:,0].to('cpu'), 'y':prior_examples[:,1].to('cpu')})\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.scatterplot(x=exmp_df['x'], y=exmp_df['y'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "172px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
