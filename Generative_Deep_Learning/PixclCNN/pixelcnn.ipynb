{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 생성모델\n",
    "\n",
    "- 학습 단계에선, Density estimation을 풀어내도록 학습\n",
    "    - $p_{model}(x) ~ p_{data}(x)$가 되도록 'un-observable pdf($p_{data}(x)$)를 estimation!\n",
    "    \n",
    "    \n",
    "- GAN : implicit density estimation\n",
    "    - $p_{model}(x)$가 explicitly 정의되지 않음\n",
    "    \n",
    "    \n",
    "- VAE : explicit density estimation\n",
    "    - $p_{model}(x)$가 explicitly '근사(approximation)됨'\n",
    "        - approximate density : intractable\n",
    "            - cannot be directly optimized\n",
    "                - So, Lower bound of likelihood optimize : maximizing the evidence lower bound(max ELBO)\n",
    "    \n",
    "    \n",
    "- AR(Auto regressive) : explicit density estimation\n",
    "    - $p_{model}(x)$가 explicitly '정의됨'\n",
    "        - tractable\n",
    "            - can be directly optimized\n",
    "                - maximizing the likelihood of training data\n",
    "                - 가능도 함수의 explicit modeling이 가능하다는 점에서 특징점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoRegressive modeling을 위한 필요조건들\n",
    "\n",
    "1. $x$ feature들에 대해 determining ordering(정해진 순서)을 가져야 함 (Auto regressive니까)\n",
    "    - AR이 주로 시계열 데이터에 사용되는 이유\n",
    "    - 시계열이 아닌 image 데이터에도 사용 가능! (임의로 순서지정)\n",
    "        - left -> right\n",
    "        - top -> bottom\n",
    "        \n",
    "    \n",
    "2. $Joint PDF = \\prod Conditional PDF$\n",
    "    - $P(x) = \\prod P(x_{i}|x_{1}, x_{2}, ..., x{i-1})$, i = determining order\n",
    "    - joint modeling 문제를 'sequence problem'으로 치환\n",
    "    - 핵심은 \"$Conditional PDF$를 어덯게 얻어내느냐\"\n",
    "        - **PDF도 함수이다. Universial Approximator인 Deep NN을 사용해 PDF를 구하자.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PixelCNN\n",
    "\n",
    "- Families of Auto-Regressive Generative model\n",
    "- 순차적으로(Sequentially) PIXEL값을 생성하는 모델\n",
    "- Convolution 연산으로 이미지 내 Pixel들의 distribution을 학습\n",
    "    - Receptive field : 컨볼루션 연산으로 얻는 receptive field는 사실 sequential problem(pixel 단위)이라는 AR과 맞지 않음\n",
    "        - 그래서 MASKing을 적용해, sequential하게 정보를 얻어갈 수 있도록 디자인!\n",
    "            - 아직 예측되지 않은, 순서가 오지 않은 pixel 정보들을 차단!\n",
    "\n",
    "<img src = \"figure_2.jpeg\" width=\"40%\" height=\"40%\">\n",
    "        \n",
    "- Masked convolutional layers\n",
    "    - mask size == fileter size\n",
    "    - mask type : A, B\n",
    "        - **type A mask**\n",
    "            - 첫 conv layer에만 적용됨(한 번만 사용되는 것)\n",
    "            - 예측하려는 pixel의 위치를 '0'으로 mask!(컨볼루션 필터 연산이 적용되지 않음)\n",
    "                - 예측하려는 위치 == filter와 mask의 center!\n",
    "            - 첫 conv에 예측하려는 위치의 input 데이터를 보지 못하게하는 효과\n",
    "            \n",
    "        - **type B mask**\n",
    "            - 이후 모든 conv layer에 적용됨\n",
    "            - 예측하려는 pixel의 위치를 '1'로 설정(컨볼루션 필터 연산이 적용됨)\n",
    "    \n",
    "<img src = \"figure_1.jpeg\" width=\"80%\" height=\"80%\">\n",
    "\n",
    "\n",
    "- Architecture\n",
    "\n",
    "<img src = \"figure_3.jpeg\" width=\"80%\" height=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMcH1_OWreV9"
   },
   "source": [
    "# PixelCNN\n",
    "\n",
    "**Author:** [ADMoreau](https://github.com/ADMoreau)<br>\n",
    "**Date created:** 2020/05/17<br>\n",
    "**Last modified:** 2020/05/23<br>\n",
    "**Description:** PixelCNN implemented in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiHC26OVreWC"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "PixelCNN is a generative model proposed in 2016 by van den Oord et al.\n",
    "(reference: [Conditional Image Generation with PixelCNN Decoders](https://arxiv.org/abs/1606.05328)).\n",
    "It is designed to generate images (or other data types) iteratively\n",
    "from an input vector where the probability distribution of prior elements dictates the\n",
    "probability distribution of later elements. In the following example, images are generated\n",
    "in this fashion, pixel-by-pixel, via a masked convolution kernel that only looks at data\n",
    "from previously generated pixels (origin at the top left) to generate later pixels.\n",
    "During inference, the output of the network is used as a probability distribution\n",
    "from which new pixel values are sampled to generate a new image\n",
    "(here, with MNIST, the pixel values range from white (0) to black (255).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch import cuda, backends\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tA06UEYreWF"
   },
   "source": [
    "## Getting the data\n",
    "\n",
    "- 왜 33% value 이하는 0으로 하고, 33%로 정했는 가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeongseobkim/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader # batchs : 5000 \n",
      "val_loader # batchs   : 834\n"
     ]
    }
   ],
   "source": [
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "n_residual_blocks = 5\n",
    "batch_size = 12\n",
    "\n",
    "# The data, split between train and test sets\n",
    "download_root = './MNIST_DATASET'\n",
    "train_dataset = MNIST(download_root, train=True, download=False)\n",
    "test_dataset = MNIST(download_root, train=False, download=False)\n",
    "\n",
    "x_train, x_test = train_dataset.data, test_dataset.data\n",
    "x_train, x_test = torch.where(x_train < (.33*256), 0, 1), torch.where(x_test < (.33*256), 0, 1)\n",
    "\n",
    "x_train = torch.cat([x_train, x_test], axis=0)\n",
    "x_train = x_train.unsqueeze(dim=1)\n",
    "x_train, x_val = data.random_split(x_train, [60000, 10000])\n",
    "\n",
    "# print(f\"x_train shape : {x_train.shape}, \\nx_train type : {type(x_train)}, \\nx_train unique val : {torch.unique(x_train)}\")\n",
    "\n",
    "train_loader = data.DataLoader(x_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = data.DataLoader(x_val, batch_size=batch_size, shuffle=True)\n",
    "print(f\"train_loader # batchs : {len(train_loader)} \\nval_loader # batchs   : {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhS0gUHCreWH"
   },
   "source": [
    "## Create two classes for the requisite Layers for the model\n",
    "\n",
    "- Mask type : 'A', 'B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first layer is the PixelCNN layer. This layer simply\n",
    "# builds on the 2D convolutional layer, but includes **masking**.\n",
    "class PixelConvLayer(nn.Conv2d):\n",
    "    \n",
    "    def __init__(self, mask_type, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        assert mask_type in ('A', 'B')\n",
    "        self.register_buffer('mask', self.weight.data.clone())\n",
    "        _, _, kH, kW = self.weight.size()\n",
    "        \n",
    "        # 일단 1로 kernel 초기화\n",
    "        self.mask.fill_(1)\n",
    "        # mask_type==B : center pixel을 1로!, 나머지 오른쪽 column은 0으로!\n",
    "        self.mask[:, :, kH // 2, kW // 2 + (mask_type == 'B'):] = 0\n",
    "        # 가장 마지막 row를 0으로 채우기\n",
    "        self.mask[:, :, kH // 2 + 1:] = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        return super(PixelConvLayer, self).forward(x)\n",
    "\n",
    "\n",
    "# Next, we build our residual block layer.\n",
    "# This is just a normal residual block, but **based on the PixelConvLayer**.\n",
    "class ResidualBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, ch_in, ch_out, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.res_block = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            \n",
    "            PixelConvLayer(mask_type = 'B', in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            \n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=1),\n",
    "            nn.ReLU(inplace=False)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.res_block(x)\n",
    "        x += residual\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**보충자료**\n",
    "\n",
    "- ```self.register_buffer('mask', self.weight.data.clone())```\n",
    "    - reference : https://discuss.pytorch.org/t/what-is-the-difference-between-register-buffer-and-register-parameter-of-nn-module/32723/7\n",
    "    - ```self.register_buffer()``` : state_dict로 저장, 학습가능한 모델의 파라미터는 아니지만, 모델의 내부 정보로써 state_dict()로 호출 될 수 있는 정보로 저장하고 싶을 때 사용. model.parameters()에는 호출되지 않기 때문에, 학습(optimization)되진 않음\n",
    "    \n",
    "\n",
    "- ```state_dict() 란 무엇인가```\n",
    "    - reference : https://tutorials.pytorch.kr/recipes/recipes/what_is_state_dict.html\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.my_tensor :  tensor([1.4851])\n",
      "model.state_dict() :  OrderedDict([('my_param', tensor([-1.0954])), ('my_buffer', tensor([-0.9836]))])\n"
     ]
    }
   ],
   "source": [
    "# 보충자료 : self.register_buffer()\n",
    "# reference : https://discuss.pytorch.org/t/what-is-the-difference-between-register-buffer-and-register-parameter-of-nn-module/32723/7\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.my_tensor = torch.randn(1)\n",
    "        self.register_buffer('my_buffer', torch.randn(1))\n",
    "        self.my_param = nn.Parameter(torch.randn(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "            return x\n",
    "\n",
    "model = MyModel()\n",
    "print(\"model.my_tensor : \", model.my_tensor)\n",
    "print(\"model.state_dict() : \", model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TERUVppwreWI"
   },
   "source": [
    "## Build the model based on the original paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super().__init__()\n",
    "        self.in_conv = PixelConvLayer(mask_type='A', in_channels=ch_in, out_channels=ch_out, kernel_size=7, padding=3)\n",
    "        self.res_blocks = nn.Sequential(*[ResidualBlock(ch_out, ch_out) for _ in range(n_residual_blocks)])\n",
    "        self.out_conv = nn.Sequential(PixelConvLayer(mask_type='B', in_channels=ch_out, out_channels=ch_out, kernel_size=1),\n",
    "                                      nn.ReLU(inplace=False),\n",
    "                                      PixelConvLayer(mask_type='B', in_channels=ch_out, out_channels=ch_out, kernel_size=1),\n",
    "                                      nn.ReLU(inplace=False))   \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=ch_out, out_channels=ch_in, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.in_conv(x)\n",
    "        x = self.res_blocks(x)\n",
    "        x = self.out_conv(x)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = PixelCNN(1, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "PixelCNN                                      --                        --\n",
       "├─PixelConvLayer: 1-1                         [1, 128, 28, 28]          6,400\n",
       "├─Sequential: 1-2                             [1, 128, 28, 28]          --\n",
       "│    └─ResidualBlock: 2-1                     [1, 128, 28, 28]          --\n",
       "│    │    └─Sequential: 3-1                   [1, 128, 28, 28]          180,608\n",
       "│    └─ResidualBlock: 2-2                     [1, 128, 28, 28]          --\n",
       "│    │    └─Sequential: 3-2                   [1, 128, 28, 28]          180,608\n",
       "│    └─ResidualBlock: 2-3                     [1, 128, 28, 28]          --\n",
       "│    │    └─Sequential: 3-3                   [1, 128, 28, 28]          180,608\n",
       "│    └─ResidualBlock: 2-4                     [1, 128, 28, 28]          --\n",
       "│    │    └─Sequential: 3-4                   [1, 128, 28, 28]          180,608\n",
       "│    └─ResidualBlock: 2-5                     [1, 128, 28, 28]          --\n",
       "│    │    └─Sequential: 3-5                   [1, 128, 28, 28]          180,608\n",
       "├─Sequential: 1-3                             [1, 128, 28, 28]          --\n",
       "│    └─PixelConvLayer: 2-6                    [1, 128, 28, 28]          16,512\n",
       "│    └─ReLU: 2-7                              [1, 128, 28, 28]          --\n",
       "│    └─PixelConvLayer: 2-8                    [1, 128, 28, 28]          16,512\n",
       "│    └─ReLU: 2-9                              [1, 128, 28, 28]          --\n",
       "├─Sequential: 1-4                             [1, 1, 28, 28]            --\n",
       "│    └─Conv2d: 2-10                           [1, 1, 28, 28]            129\n",
       "│    └─Sigmoid: 2-11                          [1, 1, 28, 28]            --\n",
       "===============================================================================================\n",
       "Total params: 942,593\n",
       "Trainable params: 942,593\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 738.99\n",
       "===============================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 14.46\n",
       "Params size (MB): 3.77\n",
       "Estimated Total Size (MB): 18.23\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "    PixelConvLayer-1          [-1, 128, 28, 28]           6,400\n",
      "            Conv2d-2          [-1, 128, 28, 28]          16,512\n",
      "              ReLU-3          [-1, 128, 28, 28]               0\n",
      "    PixelConvLayer-4          [-1, 128, 28, 28]         147,584\n",
      "              ReLU-5          [-1, 128, 28, 28]               0\n",
      "            Conv2d-6          [-1, 128, 28, 28]          16,512\n",
      "              ReLU-7          [-1, 128, 28, 28]               0\n",
      "     ResidualBlock-8          [-1, 128, 28, 28]               0\n",
      "            Conv2d-9          [-1, 128, 28, 28]          16,512\n",
      "             ReLU-10          [-1, 128, 28, 28]               0\n",
      "   PixelConvLayer-11          [-1, 128, 28, 28]         147,584\n",
      "             ReLU-12          [-1, 128, 28, 28]               0\n",
      "           Conv2d-13          [-1, 128, 28, 28]          16,512\n",
      "             ReLU-14          [-1, 128, 28, 28]               0\n",
      "    ResidualBlock-15          [-1, 128, 28, 28]               0\n",
      "           Conv2d-16          [-1, 128, 28, 28]          16,512\n",
      "             ReLU-17          [-1, 128, 28, 28]               0\n",
      "   PixelConvLayer-18          [-1, 128, 28, 28]         147,584\n",
      "             ReLU-19          [-1, 128, 28, 28]               0\n",
      "           Conv2d-20          [-1, 128, 28, 28]          16,512\n",
      "             ReLU-21          [-1, 128, 28, 28]               0\n",
      "    ResidualBlock-22          [-1, 128, 28, 28]               0\n",
      "           Conv2d-23          [-1, 128, 28, 28]          16,512\n",
      "             ReLU-24          [-1, 128, 28, 28]               0\n",
      "   PixelConvLayer-25          [-1, 128, 28, 28]         147,584\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "           Conv2d-27          [-1, 128, 28, 28]          16,512\n",
      "             ReLU-28          [-1, 128, 28, 28]               0\n",
      "    ResidualBlock-29          [-1, 128, 28, 28]               0\n",
      "           Conv2d-30          [-1, 128, 28, 28]          16,512\n",
      "             ReLU-31          [-1, 128, 28, 28]               0\n",
      "   PixelConvLayer-32          [-1, 128, 28, 28]         147,584\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "           Conv2d-34          [-1, 128, 28, 28]          16,512\n",
      "             ReLU-35          [-1, 128, 28, 28]               0\n",
      "    ResidualBlock-36          [-1, 128, 28, 28]               0\n",
      "   PixelConvLayer-37          [-1, 128, 28, 28]          16,512\n",
      "             ReLU-38          [-1, 128, 28, 28]               0\n",
      "   PixelConvLayer-39          [-1, 128, 28, 28]          16,512\n",
      "             ReLU-40          [-1, 128, 28, 28]               0\n",
      "           Conv2d-41            [-1, 1, 28, 28]             129\n",
      "          Sigmoid-42            [-1, 1, 28, 28]               0\n",
      "================================================================\n",
      "Total params: 942,593\n",
      "Trainable params: 942,593\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 30.64\n",
      "Params size (MB): 3.60\n",
      "Estimated Total Size (MB): 34.24\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torchsummary\n",
    "\n",
    "torchsummary.summary(model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "criterion = nn.BCELoss()\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device :\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0/50 \n",
      "train loss : 0.0932(sec : 36.4), val loss : 0.0872(sec : 1.8)\n",
      "Epoch : 1/50 \n",
      "train loss : 0.0862(sec : 36.4), val loss : 0.0857(sec : 1.9)\n",
      "Epoch : 2/50 \n",
      "train loss : 0.0848(sec : 36.5), val loss : 0.0842(sec : 1.9)\n",
      "Epoch : 3/50 \n",
      "train loss : 0.0840(sec : 36.7), val loss : 0.0839(sec : 1.9)\n",
      "Epoch : 4/50 \n",
      "train loss : 0.0834(sec : 36.7), val loss : 0.0835(sec : 1.9)\n",
      "Epoch : 5/50 \n",
      "train loss : 0.0830(sec : 38.5), val loss : 0.0832(sec : 1.9)\n",
      "Epoch : 6/50 \n",
      "train loss : 0.0827(sec : 38.6), val loss : 0.0833(sec : 1.9)\n",
      "Epoch : 7/50 \n",
      "train loss : 0.0824(sec : 37.0), val loss : 0.0830(sec : 1.9)\n",
      "Epoch : 8/50 \n",
      "train loss : 0.0822(sec : 37.0), val loss : 0.0830(sec : 1.9)\n",
      "Epoch : 9/50 \n",
      "train loss : 0.0820(sec : 37.3), val loss : 0.0828(sec : 2.0)\n",
      "Epoch : 10/50 \n",
      "train loss : 0.0818(sec : 39.6), val loss : 0.0827(sec : 2.0)\n",
      "Epoch : 11/50 \n",
      "train loss : 0.0816(sec : 39.0), val loss : 0.0829(sec : 1.9)\n",
      "Epoch : 12/50 \n",
      "train loss : 0.0815(sec : 37.1), val loss : 0.0827(sec : 1.9)\n",
      "Epoch : 13/50 \n",
      "train loss : 0.0813(sec : 37.1), val loss : 0.0827(sec : 1.9)\n",
      "Epoch : 14/50 \n",
      "train loss : 0.0812(sec : 37.2), val loss : 0.0828(sec : 1.9)\n",
      "Epoch : 15/50 \n",
      "train loss : 0.0810(sec : 37.1), val loss : 0.0828(sec : 1.9)\n",
      "Epoch : 16/50 \n",
      "train loss : 0.0809(sec : 37.2), val loss : 0.0827(sec : 1.9)\n",
      "Epoch : 17/50 \n",
      "train loss : 0.0808(sec : 37.3), val loss : 0.0828(sec : 1.9)\n",
      "Epoch : 18/50 \n",
      "train loss : 0.0807(sec : 37.0), val loss : 0.0828(sec : 1.9)\n",
      "Epoch : 19/50 \n",
      "train loss : 0.0806(sec : 37.0), val loss : 0.0827(sec : 1.9)\n",
      "Epoch : 20/50 \n",
      "train loss : 0.0805(sec : 37.1), val loss : 0.0829(sec : 1.9)\n",
      "Epoch : 21/50 \n",
      "train loss : 0.0804(sec : 37.1), val loss : 0.0830(sec : 1.9)\n",
      "Epoch : 22/50 \n",
      "train loss : 0.0803(sec : 37.0), val loss : 0.0832(sec : 1.9)\n",
      "Epoch : 23/50 \n",
      "train loss : 0.0802(sec : 37.2), val loss : 0.0830(sec : 1.9)\n",
      "Epoch : 24/50 \n",
      "train loss : 0.0801(sec : 37.3), val loss : 0.0830(sec : 1.9)\n",
      "Epoch : 25/50 \n",
      "train loss : 0.0801(sec : 38.5), val loss : 0.0831(sec : 1.9)\n",
      "Epoch : 26/50 \n",
      "train loss : 0.0800(sec : 40.4), val loss : 0.0831(sec : 1.9)\n",
      "Epoch : 27/50 \n",
      "train loss : 0.0799(sec : 37.1), val loss : 0.0832(sec : 1.9)\n",
      "Epoch : 28/50 \n",
      "train loss : 0.0799(sec : 37.3), val loss : 0.0832(sec : 1.9)\n",
      "Epoch : 29/50 \n",
      "train loss : 0.0798(sec : 37.2), val loss : 0.0833(sec : 1.9)\n",
      "Epoch : 30/50 \n",
      "train loss : 0.0797(sec : 37.2), val loss : 0.0832(sec : 1.9)\n",
      "Epoch : 31/50 \n",
      "train loss : 0.0797(sec : 37.0), val loss : 0.0833(sec : 1.9)\n",
      "Epoch : 32/50 \n",
      "train loss : 0.0796(sec : 37.1), val loss : 0.0835(sec : 1.9)\n",
      "Epoch : 33/50 \n",
      "train loss : 0.0795(sec : 37.1), val loss : 0.0835(sec : 1.9)\n",
      "Epoch : 34/50 \n",
      "train loss : 0.0795(sec : 37.2), val loss : 0.0835(sec : 1.9)\n",
      "Epoch : 35/50 \n",
      "train loss : 0.0795(sec : 37.1), val loss : 0.0835(sec : 1.9)\n",
      "Epoch : 36/50 \n",
      "train loss : 0.0794(sec : 37.3), val loss : 0.0836(sec : 1.9)\n",
      "Epoch : 37/50 \n",
      "train loss : 0.0793(sec : 36.9), val loss : 0.0836(sec : 1.9)\n",
      "Epoch : 38/50 \n",
      "train loss : 0.0793(sec : 37.0), val loss : 0.0837(sec : 1.9)\n",
      "Epoch : 39/50 \n",
      "train loss : 0.0793(sec : 37.2), val loss : 0.0836(sec : 1.9)\n",
      "Epoch : 40/50 \n",
      "train loss : 0.0792(sec : 37.2), val loss : 0.0836(sec : 1.9)\n",
      "Epoch : 41/50 \n",
      "train loss : 0.0792(sec : 37.3), val loss : 0.0836(sec : 1.9)\n",
      "Epoch : 42/50 \n",
      "train loss : 0.0791(sec : 37.2), val loss : 0.0837(sec : 1.9)\n",
      "Epoch : 43/50 \n",
      "train loss : 0.0791(sec : 37.1), val loss : 0.0837(sec : 1.9)\n",
      "Epoch : 44/50 \n",
      "train loss : 0.0791(sec : 37.1), val loss : 0.0838(sec : 1.9)\n",
      "Epoch : 45/50 \n",
      "train loss : 0.0790(sec : 37.2), val loss : 0.0836(sec : 1.9)\n",
      "Epoch : 46/50 \n",
      "train loss : 0.0790(sec : 37.1), val loss : 0.0839(sec : 1.9)\n",
      "Epoch : 47/50 \n",
      "train loss : 0.0789(sec : 37.1), val loss : 0.0838(sec : 1.9)\n",
      "Epoch : 48/50 \n",
      "train loss : 0.0789(sec : 37.0), val loss : 0.0837(sec : 1.9)\n",
      "Epoch : 49/50 \n",
      "train loss : 0.0789(sec : 37.2), val loss : 0.0839(sec : 1.9)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "errs_train, errs_val = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.to(device)\n",
    "    losses, losses_val = float(), float()\n",
    "\n",
    "    time_tr = time.time()\n",
    "    model.train()\n",
    "    for x in train_loader:\n",
    "\n",
    "        x = x.float().to(device)\n",
    "        target = x.data.float().to(device)\n",
    "        \n",
    "        pred = model(x)\n",
    "\n",
    "        loss = criterion(pred, target)\n",
    "        \n",
    "        losses += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    time_tr = time.time() - time_tr\n",
    "    errs_train.append(losses/len(train_loader))\n",
    "    \n",
    "\n",
    "    time_ev = time.time()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x_val in val_loader:\n",
    "\n",
    "            x_val = x_val.float().to(device)\n",
    "            target_val = x_val.data.float().to(device)\n",
    "            pred_val = model(x_val)\n",
    "\n",
    "            loss_val = criterion(pred_val, target_val)\n",
    "            \n",
    "            losses_val += loss_val.item()\n",
    "            \n",
    "        time_ev = time.time() - time_ev\n",
    "        errs_val.append(losses_val/len(val_loader))\n",
    "        \n",
    "    print(f\"Epoch : {epoch}/{num_epochs} \\ntrain loss : {errs_train[-1]:.4f}(sec : {time_tr:.1f}), val loss : {errs_val[-1]:.4f}(sec : {time_ev:.1f})\")     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'pixel_cnn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PixelCNN(\n",
       "  (in_conv): PixelConvLayer(1, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "  (res_blocks): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (res_block): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): PixelConvLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU()\n",
       "        (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (res_block): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): PixelConvLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU()\n",
       "        (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (res_block): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): PixelConvLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU()\n",
       "        (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (3): ResidualBlock(\n",
       "      (res_block): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): PixelConvLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU()\n",
       "        (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (4): ResidualBlock(\n",
       "      (res_block): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): PixelConvLayer(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU()\n",
       "        (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (5): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out_conv): Sequential(\n",
       "    (0): PixelConvLayer(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): PixelConvLayer(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PixelCNN(1, 128)\n",
    "model.load_state_dict(torch.load('pixel_cnn.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del train_loader\n",
    "del val_loader\n",
    "del loss\n",
    "del loss_val\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZ9QY51xreWJ"
   },
   "source": [
    "## Demonstration\n",
    "\n",
    "The PixelCNN cannot generate the full image at once. Instead, it must generate each pixel in\n",
    "order, append the last generated pixel to the current image, and feed the image back into the\n",
    "model to repeat the process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch, channels, rows, cols : (3, 1, 28, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:13<00:00,  2.00it/s]\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Create an empty array of pixels.\n",
    "batch = 3\n",
    "pixels = torch.zeros(size=(batch,) + (1, 28, 28))\n",
    "batch, channels, rows, cols = pixels.shape\n",
    "\n",
    "print(f\"batch, channels, rows, cols : {batch, channels, rows, cols}\")\n",
    "\n",
    "# Iterate over the pixels because generation has to be done sequentially pixel by pixel.\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for row in tqdm(range(rows)):\n",
    "        for col in range(cols):\n",
    "            for channel in range(channels):\n",
    "\n",
    "                # Feed the whole array and retrieving the pixel value probabilities for the next\n",
    "                # pixel.\n",
    "                model.to(\"cpu\")   # because of cuda 'out of memory' error\n",
    "                pred = model(pixels.float())\n",
    "                probs = pred[:, channel, row, col]\n",
    "\n",
    "                # Use the probabilities to pick pixel values and append the values to the image\n",
    "                # frame.\n",
    "                pixels[:, channel, row, col] = torch.ceil(\n",
    "                    probs - torch.rand(size=probs.shape)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAAh0lEQVR4nO2UwQ6AIAxDwfj/v1wPeiC4raMQTYw9ku1RurBSfn1BAACMdlWP1ddVu9LUliF6h1lo0JznGk7nFUGHcoygLUiY+wWhFS066Z1nKoTw+KBOCcm+4VSYEodqiqCaTQKV5UJlmxF0RnxJdzYzL9iTl3ub2+Ry6JofNUQxixcM6s49AFrCPxB1rdDaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAAo0lEQVR4nO1Vyw6AIAwD4///8jyYKCndU7wYe4St617Q2o8vQ0RERLvtQQrXpvebalvCCNhrdKcuzYCkb0sb0wTj6wrTt9UB4xwjqpS6URfHMtscmDDe/Yg6A1b3I6A51RUZpa+QzuqgXP5GZRlbVmlw2tJKIQZtVIJUG975/JFSDemajnWkr0nLKg1u2ivpkzUtvFLW8NMRWfmdjB8GNMQNcwB3dWALFb7J1AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAAj0lEQVR4nO1Uyw6AIAwDw///cj0YDO7thMQDPcEoZSuwUjY2/o/KQwAejCpwbBwug5wxR3R+pglFQTRRrC86BUtE2zgJ1s5pxHorU35LAMSDSXC9p2Nq4xgd4ipHI/P4w7yYohvJ8jVzo6LGZm3JEeUdS/TnxZNK98Cop7di5IOool86SyhTrXAtnv9RhsUnzOE8JnKYepYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Iterate over the generated images and plot them with matplotlib.\n",
    "for i, pic in enumerate(pixels):\n",
    "    pic = pic.detach().numpy()\n",
    "    pic = torch.from_numpy(pic)\n",
    "    save_image(pic, f\"generated_img_{i}_torch.png\")\n",
    "    \n",
    "    \n",
    "display(Image(\"generated_img_0_torch.png\"))\n",
    "display(Image(\"generated_img_1_torch.png\"))\n",
    "display(Image(\"generated_img_2_torch.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
