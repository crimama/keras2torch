{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Author**: Jaehyuk Heo\n",
    "- **Paper**: LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS [ [link](https://arxiv.org/pdf/2106.09685.pdf) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <img width='400' src='https://user-images.githubusercontent.com/37654013/168552268-c764cb0c-2684-4082-a633-ba2d8cf340a3.png'>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to install**\n",
    "---\n",
    "```bash\n",
    "pip install loralib\n",
    "```\n",
    "---\n",
    "\n",
    "**Example of LoRA Linear**\n",
    "\n",
    "---\n",
    "```python\n",
    "class LoRALayer():\n",
    "    def __init__(\n",
    "        self, \n",
    "        r: int, \n",
    "        lora_alpha: int, \n",
    "        lora_dropout: float,\n",
    "        merge_weights: bool,\n",
    "    ):\n",
    "        self.r = r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        # Optional dropout\n",
    "        if lora_dropout > 0.:\n",
    "            self.lora_dropout = nn.Dropout(p=lora_dropout)\n",
    "        else:\n",
    "            self.lora_dropout = lambda x: x\n",
    "        # Mark the weight as unmerged\n",
    "        self.merged = False\n",
    "        self.merge_weights = merge_weights\n",
    "        \n",
    "\n",
    "class Linear(nn.Linear, LoRALayer):\n",
    "    # LoRA implemented in a dense layer\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_features: int, \n",
    "        out_features: int, \n",
    "        r: int = 0, \n",
    "        lora_alpha: int = 1, \n",
    "        lora_dropout: float = 0.,\n",
    "        fan_in_fan_out: bool = False, # Set this to True if the layer to replace stores weight like (fan_in, fan_out)\n",
    "        merge_weights: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n",
    "        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n",
    "                           merge_weights=merge_weights)\n",
    "\n",
    "        self.fan_in_fan_out = fan_in_fan_out\n",
    "        # Actual trainable parameters\n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Parameter(self.weight.new_zeros((r, in_features)))\n",
    "            self.lora_B = nn.Parameter(self.weight.new_zeros((out_features, r)))\n",
    "            self.scaling = self.lora_alpha / self.r\n",
    "            # Freezing the pre-trained weight matrix\n",
    "            self.weight.requires_grad = False\n",
    "        self.reset_parameters()\n",
    "        if fan_in_fan_out:\n",
    "            self.weight.data = self.weight.data.T\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.Linear.reset_parameters(self)\n",
    "        if hasattr(self, 'lora_A'):\n",
    "            # initialize A the same way as the default for nn.Linear and B to zero\n",
    "            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        def T(w):\n",
    "            return w.T if self.fan_in_fan_out else w\n",
    "        nn.Linear.train(self, mode)\n",
    "        if self.merge_weights and self.merged:\n",
    "            # Make sure that the weights are not merged\n",
    "            if self.r > 0:\n",
    "                self.weight.data -= T(self.lora_B @ self.lora_A) * self.scaling\n",
    "            self.merged = False\n",
    "    \n",
    "    def eval(self):\n",
    "        def T(w):\n",
    "            return w.T if self.fan_in_fan_out else w\n",
    "        nn.Linear.eval(self)\n",
    "        if self.merge_weights and not self.merged:\n",
    "            # Merge the weights and mark it\n",
    "            if self.r > 0:\n",
    "                self.weight.data += T(self.lora_B @ self.lora_A) * self.scaling\n",
    "            self.merged = True\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        def T(w):\n",
    "            return w.T if self.fan_in_fan_out else w\n",
    "        if self.r > 0 and not self.merged:\n",
    "            result = F.linear(x, T(self.weight), bias=self.bias)\n",
    "            if self.r > 0:\n",
    "                result += (self.lora_dropout(x) @ self.lora_A.T @ self.lora_B.T) * self.scaling\n",
    "            return result\n",
    "        else:\n",
    "            return F.linear(x, T(self.weight), bias=self.bias)\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import loralib as lora\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from timm import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    lora_r = 8\n",
    "    lora_alpha = 8\n",
    "    \n",
    "args = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.nn.linear.weight.size():  torch.Size([32, 16])\n"
     ]
    }
   ],
   "source": [
    "torch_linear = nn.Linear(16, 32)\n",
    "\n",
    "print('torch.nn.linear.weight.size(): ',torch_linear.weight.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result size:  torch.Size([1, 32])\n"
     ]
    }
   ],
   "source": [
    "result = nn.functional.linear(torch.randn(1,16), torch_linear.weight)\n",
    "print('result size: ',result.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load a model\n"
     ]
    }
   ],
   "source": [
    "model = create_model(\n",
    "    'vit_base_patch16_224', \n",
    "    num_classes = 100, \n",
    "    pretrained  = True\n",
    ")\n",
    "\n",
    "print('load a model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of total parameters:  85875556\n",
      "# of trainable parameters:  85875556\n"
     ]
    }
   ],
   "source": [
    "total_params = np.sum([p.numel() for p in model.parameters()])\n",
    "trainable_params = 0\n",
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "        trainable_params += p.numel()\n",
    "\n",
    "print('# of total parameters: ',total_params)\n",
    "print('# of trainable parameters: ',trainable_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lora_A.size():  torch.Size([8, 16])\n",
      "lora_B.size():  torch.Size([32, 8])\n",
      "(lora_A.T @ lora_B.T).size():  torch.Size([16, 32])\n",
      "result size:  torch.Size([1, 32])\n"
     ]
    }
   ],
   "source": [
    "lora_linear = lora.Linear(16, 32, r=args.lora_r, lora_alpha=args.lora_alpha)\n",
    "\n",
    "print('lora_A.size(): ',lora_linear.lora_A.size())\n",
    "print('lora_B.size(): ',lora_linear.lora_B.size())\n",
    "\n",
    "print('(lora_A.T @ lora_B.T).size(): ',(lora_linear.lora_A.T @ lora_linear.lora_B.T).size())\n",
    "\n",
    "result = torch.randn(1,16) @ lora_linear.lora_A.T @ lora_linear.lora_B.T\n",
    "print('result size: ',result.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load a model\n"
     ]
    }
   ],
   "source": [
    "model_lora = create_model(\n",
    "    'vit_base_patch16_224', \n",
    "    num_classes = 100, \n",
    "    apply_lora  = True, \n",
    "    lora_r      = 8, \n",
    "    lora_alpha  = 8, \n",
    "    pretrained  = True\n",
    ")\n",
    "\n",
    "lora.mark_only_lora_as_trainable(model_lora)\n",
    "print('load a model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of total parameters:  86317924\n",
      "# of trainable parameters:  442368\n"
     ]
    }
   ],
   "source": [
    "total_params = np.sum([p.numel() for p in model_lora.parameters()])\n",
    "trainable_params = 0\n",
    "for p in model_lora.parameters():\n",
    "    if p.requires_grad:\n",
    "        trainable_params += p.numel()\n",
    "\n",
    "print('# of total parameters: ',total_params)\n",
    "print('# of trainable parameters: ',trainable_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
